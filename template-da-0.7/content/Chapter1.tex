\chapter{Benchmarking}
Vergleichskriterien für Betriebssysteme
\begin{itemize}
	\item Performanz
	\item Sicherheit
	\item Ressourcen-/Speicherverbrauch
	\item Speicherverwaltung
\end{itemize}

\section{Performanz}
\begin{enumerate}
	\item Latenzzeiten/Jitter
		\begin{enumerate}
			\item Interrupt durch Taster $ \rightarrow $ Aufblinken von LED (die Zeit, die dasAufblinken benötigt, kann gemessen und abgezogen werden, so dass nur die Zeit vom Drücken des Tasters bis zum Ausführen der ISR bleibt)
			\item Andere Interruptquellen? ( $ \rightarrow $ z.B CAN, Ethernet, SPI, ...)
			\item Verschiedene Taktzeiten von FreeRTOS
		\end{enumerate}
	\item Durchsatz an Daten
		\begin{enumerate}
			\item Ethernet
			\item CAN
			\item SPI
		\end{enumerate}	
	\item Bootzeit
		\begin{enumerate}
			\item Was hat Einwirkungen auf die Bootzeit?
			\item Indikatoren $ \rightarrow $ Wann ist das System hochgefahren?
			\item Bestimmtes Programm wird gestartet $ \rightarrow $ z.B. Aufleuchten von LED
			\item Bestimmte Programme können den Bootvorgang aufzeichen (Bootchart)  
		\end{enumerate}			
\end{enumerate}

\subsection{Latenzzeiten von Interrupts}
Für die Latenzzeit von Interrupts soll ein Interrupt von einer externen Quelle ausgelöst werden und dann wird gemessen, wann die Interruptserviceroutine betreten wird. Konkret wird periodisch ein GPIO-Interrupt durch einen Signalgenerator in Hardware ausgelöst. Die GPIO wird über das EMIO-Interface angebunden. In der dazugehörigen ISR wird eine LED angeschaltet. Es wird die Zeit zwischen dem Setzen des Interruptsignals und aufblinken der LED gemessen. Von dieser Zeit muss abgezogen werden, wie lange das Anschalten der LED und die Zeitmessung an sich dauert. Grundsätzlich wird jede Messung 1024 Mal durchgeführt. Für die Zusatzmessungen sind die Durchschnittszeiten interessant. Für die Hauptmessung ist zusätzlich der Worst-Case-Fall zu beachten.
 
\subsubsection{FreeRTOS}
Bei FreeRTOS werden die Interrupts unabhängig vom Betriebssystem verwaltet. Der EMIO-GPIO-Interrupt hat nach dem System-Timer die höchste Priorität. 
\\\\Axi-Timer: 50 MHz

Ergebnis: 

\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|l|}
			\hline
			 Anz. Messungen &  19100 \\	
			\hline
			 Durchschnittswert & 754,8ns \\
			\hline
			 Standardabweichung &  8,5977ns (19.31 ns) \\
			\hline
			 Minimalwert & 740ns \\
			\hline	
			 Maximalwert &  780ns \\
			\hline
			 Durchschnittswert LED-Anschalten &  700-740ns (130 ns)\\
			\hline
			 Durchschnittswert Timer-Overhead &  520ns (9ns) \\
			\hline				
			LED-Overhead bei ISR-Messung & 220ns (121 ns)\\
			\hline		
			Durchschnitt - Overhead & 633,8ns \\
			\hline	
			Durchschnitt - Overhead (Taktzyklen) & 422 Zyklen\\
			\hline								
		\end{tabular}
		\label{t_isr_no_task}
\end{table*}

mit Task:
\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|l|}
			\hline
			 Anz. Messungen &  19100 \\	
			\hline
			 Durchschnittswert & 754,8ns \\
			\hline
			 Standardabweichung &  8,5977ns (19.31 ns) \\
			\hline
			 Minimalwert & 719ns \\
			\hline	
			 Maximalwert & 840ns \\
			\hline
			 Durchschnittswert LED-Anschalten &  700-740ns (130 ns)\\
			\hline
			 Durchschnittswert Timer-Overhead &  520ns (9ns) \\
			\hline				
			LED-Overhead bei ISR-Messung & 220ns (121 ns)\\
			\hline		
			Durchschnitt - Overhead & 633,8ns \\
			\hline	
			Durchschnitt - Overhead (Taktzyklen) & 422 Zyklen\\
			\hline								
		\end{tabular}
		\label{t_isr_task}
\end{table*}

\subsubsection{LinuxRT}
Messung mit wmb():
Ergebnis: 
\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|l|}
			\hline
			 Anz. Messungen &  19160 \\	
			\hline
			 Durchschnittswert & 13,569us \\
			\hline
			 Standardabweichung &  1,4412us \\
			\hline
			 Minimalwert & 8us \\
			\hline	
			 Maximalwert &  28,8us \\
			\hline
			 Durchschnittswert LED-Anschalten &   \\
			\hline
			 Durchschnittswert Timer-Overhead &   \\
			\hline				
			LED-Overhead bei ISR-Messung &  \\
			\hline		
			Durchschnitt - Overhead & s \\
			\hline	
			Durchschnitt - Overhead (Taktzyklen) & \\
			\hline								
		\end{tabular}
		\label{t_isr_no_task_wmb}
\end{table*}

Messung ohne wmb():
Ergebnis: 
\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|l|}
			\hline
			 Anz. Messungen &  19130 \\	
			\hline
			 Durchschnittswert & 13,406us \\
			\hline
			 Standardabweichung &  1,3419us \\
			\hline
			 Minimalwert & 8,6us \\
			\hline	
			 Maximalwert &  32us \\
			\hline
			 Durchschnittswert LED-Anschalten &   \\
			\hline
			 Durchschnittswert Timer-Overhead &   \\
			\hline				
			LED-Overhead bei ISR-Messung &  \\
			\hline		
			Durchschnitt - Overhead & s \\
			\hline	
			Durchschnitt - Overhead (Taktzyklen) & \\
			\hline								
		\end{tabular}
		\label{t_isr_task_wmb}
\end{table*}

Messung For-Loops in Zyklen:
\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|l|l|}
			\hline	
			For-Loop 0 & Runden 16 &  180  \\
			\hline	
			For-Loop 1 & Runden 32 &  304  \\
			\hline	
			For-Loop 2 & Runden 64 &  533 \\
			\hline	
			For-Loop 3 & Runden 128 &  1045  \\
			\hline	
			For-Loop 4 & Runden 264 &  2133  \\
			\hline	
			For-Loop 5 & Runden 512 &  4117  \\
			\hline	
			For-Loop 6 & Runden 1024 &  8213  \\
			\hline	
			For-Loop 7 & Runden 2048 &  16405  \\
			\hline	
			For-Loop 8 & Runden 4096 &  32789  \\
			\hline	
			For-Loop 9 & Runden 8192 & 65557  \\
			\hline	
		\end{tabular}
		\label{t_for_loop}
\end{table*}


Messung While-Loops in Zyklen:
\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|l|l|}
			\hline	
			Loop 0 & Runden 16 &  171  \\
			\hline	
			Loop 1 & Runden 32 & 301 \\
			\hline	
			Loop 2 & Runden 64 &  535 \\
			\hline	
			Loop 3 & Runden 128 &  1047  \\
			\hline	
			Loop 4 & Runden 264 &  2135  \\
			\hline	
			Loop 5 & Runden 512 &  4119  \\
			\hline	
			Loop 6 & Runden 1024 &  8215  \\
			\hline	
			Loop 7 & Runden 2048 &  16407  \\
			\hline	
			Loop 8 & Runden 4096 &  32791  \\
			\hline	
			Loop 9 & Runden 8192 & 65559  \\
			\hline	
		\end{tabular}
		\label{t_while_loop}
\end{table*}
Overhead Timer-Messung: $ 6 Zyklen $
\\Overhead Registerzuweisung: $ 19 - 6 = 13 Zyklen = 19 ns $
\\Messung der Registerzuweisung mit wmb()\footnote{wmb(These functions insert hardware memory barriers in the compiled instruction flow; their actual instantiation is platform dependent. An rmb (read memory barrier) guarantees that any reads appearing before the barrier are completed prior to the execution of any subsequent read. wmb guarantees ordering in write operations, and the mbinstruction guarantees both. Each of these functions is a superset of barrier). Das bedeutet, dass die Schreiboperationen auf die Hardware bis zu dieser Barriere abgeschlossen sein müssen und man davon ausgehen kann, dass der Hardwarezugriff bereits erfolgt ist. Somit kann man messen, wie lange ein Hardwarezugriff zum Beschreiben einer LED dauert und diese Zeit von der Gesamtzeit abziehen. Um ein sinnvolles Ergebnis zu erhalten, sollte auch die Messung mit wmb() arbeiten. } 

\subsection{Unterbrechung von Task durch ISR}
Ein Task läuft und speichert in einer While-Schleife immer die aktuelle Zeit. Er wird durch eine ISR unterbrochen. Sobald der Task wieder anläuft, wird wieder die Zeit gemessen. Die gesuchte Zeit ist die Different aus Start- und Endzeit.
\\\\Die übliche Methode für Interrupt Service Routinen in FreeRTOS ist, einen hochpriorisierten Task laufen zu lassen, der an einem binären Semaphor blockiert. Wird die ISR aufgerufen, wird dieser Task wieder deblockiert. 

\section{RT-Features}
	\begin{enumerate}
		\item Welche Unterschiede/Gemeinsamkeiten gibt es zwischen FreeRTOS und Linux?
		\item Prioritäten 
		%\item Semaphore
		%\item Message Passing (s. Semaphore)
		%\item Queues (s. Semaphore)
		\item Flags (s. Semaphore)
		\item Posix-Features in Linux
	\end{enumerate}

\subsection{RT-Features von Linux/Posix}
Für Linux gibt es mehrere IPC Möglichkeiten: Die POSIX IPC und die System V IPC. Nach Möglichkeit wird die POSIX IPC benutzt, aber da nicht alle POSIX features auf dem System unterstützt werden, muss dafür auch auf die System V IPC zurückgegriffen werden.
\subsubsection{Threads}
Threads in Unix sind Teile von Prozessen. Allerdings hat ein Thread einen eigenen Stack Pointer, eigene Register, Scheduling Properties, Signale und andere Daten. Ein Thread existiert, solange der Elternprozess existiert. Ein Prozess kann mehrere Threads haben. Es kann Datenaustausch von Threads im Rahmen eines Prozesses geben. Ein Thread verbraucht deutlich weniger Ressourcen als ein Prozess. Inter-Thread-Kommunikation ist deutlich schneller, weil alles in einem Adressraum stattfindet. Bei Inter-Prozess-Kommunikation ist mindestens ein Kopiervorgang von Prozess zu Prozess erforderlich. Um pthreads benutzen zu können, muss die pthread-Bibliothek eingebunden werden (über -lpthread) und um zum Beispiel Prioritätsvererbung für Mutexe zu verwenden, muss zusätzlich mit dem Compilerflag $ \textit{-D\_GNU\_SOURCE=1} $ kompiliert werden.  
\subsubsection{Mutexes}
An Mutexen kann geblockt werden, aber es kann auch ausprobiert werden, ob sie bereits gelockt sind, und dann kann was anderes gemacht werden.
\subsubsection{Conditions}
Conditions werden im Zusammenhang mit Mutexen benutzt und dienen zur Synchronisation von mehreren Threads, die Datenabhängig sind. Zur Benutzung: An einer Condition kann gewartet werden. Zuvor muss ein bestimmter Mutex genommen worden sein. Wenn man den Befehl \textit{pthread\_cond\_wait} ausführt, dann wird damit gewartet und der Mutex automatisch losgelassen. Ein anderer Thread kann sich denselben Mutex holen und dann eine bestimmte Datenverarbeitung an einer Variable durchführen. Wenn dadurch die Bedingung erfüllt wird, ruft dieser Thread die Funktion \textit{pthread\_cond\_signal}, die den anderen Thread aufweckt, sobald der Mutex losgelassen wurde.
\subsubsection{Join}
Threads können \textit{gejoint} werden. Dieses ist ein Synchronisationsmechanismus von PThreads. Wird \textit{pthread\_join} aufgerufen, blockiert der aktuelle Thread, bis der zu synchronisierende Thread beendet ist.
\subsubsection{Message queues}
\textit{mqd\_t}. Eine Queue muss erst erstellt werden. \textit{mq\_send} und \textit{mq\_receive} können blockend und nicht blockend aufgerufen werden, indem das Flag \textit{O\_NONBLOCK} gesetzt wird. Tasks können außerdem darüber informiert werden, dass eine Message in der Queue abgelegt wurde. Dieses funktioniert über \textit{mq\_notify}. Darüber kann ein Handler installiert werden, der ausgeführt wird, wenn eine neue Nachricht empfangen wird. Wenn ein anderer Task mit \textit{mq\_receive} an der Queue blockiert, dann wird kein Signal verschickt und der Handler nicht ausgelöst. The library \textit{rt} muss beim Kompilieren gelinkt werden.
\subsubsection{Scheduling}
\textit{schedPxLib}
\textit{sched\_getScheduler}, gibt entweder SCHED\_FIFO oder SCHED\_RR zurück.
\begin{itemize}
	\item \textit{sched\_get\_priority\_max}  
	\item \textit{sched\_set\_priority\_max}
	\item \textit{sched\_rr\_get\_interval}
	\item \textit{sched\_yield}
	\item \textit{sched\_rr\_get\_interval}
	\item \textit{pthread\_setschedprio}
	\item \textit{pthread\_kill}
\end{itemize}


\subsubsection{Semaphores}
\textit{semPxLib}. Posix Semaphores sind zählende Semaphore. Die unterstützen Funktionen sind Prioritätsvererbung, rekurive Semaphore, Timeouts, ... . Posix Mutexes und Condition variables wurden implementiert, indem standardmäßige Semaphore verwendet wurden.


\subsection{RT-Features von FreeRTOS}
Relevante Features:
\subsubsection{Tasks}
Tasks unter FreeRTOS können mit verschiedenen Prioritäten erstellt werden. Der idleTask hat immer die niedrigste Priorität. Tasks haben verschiedene Zustände:
\begin{itemize}
\item Running: Task wird ausgeführt. Es kann zur Zeit nur einen einzigen Task geben, der gerade ausgeführt wird.
\item Ready: Wartet darauf, ausgeführt zu werden, da ein anderer Task gerade vom Scheduler gescheduled wurde
\item Blocked: Task wartet auf ein Ereignis und wird nicht ausgeführt. Grund kann ein Delay oder das Warten an einer Queue oder einem Semaphor sein.
\item Suspended: Ein Task wurde von einem anderen Task oder sich selber suspendiert. Dieser Status kann nur durch einen Aufruf der Funktion \textit{xTaskResume} fortgesetzt werden.
\end{itemize}

\subsubsection{Scheduling Policy}
Die Policy wird \textit{Fixed Priority Preemptive Scheduling} genannt. Jedem Task wird eine eigene Priorität zugewiesen, wobei Tasks auch die gleiche Priorität haben können. Für jede Priorität existiert eine eigene Liste.


\subsubsection{Queues}
Queues werden benutzt, um Nachrichten zwischen Tasks auszutauschen. Von einer Queue kann zerstörend oder nicht zerstörend gelesen werden und drauf geschrieben werden. Wenn ein Task an einer Queue wartet, kann er für eine bestimmte Zeit blockiert werde. Eine Queue kann mit unterschiedlichen Größen erzeugt werden. Es gibt auch sogenannte Queue-Sets, die es ermöglichen an mehreren Queue oder auch Semaphoren zu warten.

\subsubsection{Semaphore}
Es gibt drei verschiedene Arten von Semaphoren (s. \ref{semaphore_shuffling_time}), Mutexe, binäre Semaphore und Counting Semaphors. Semaphore werden als Queue implementiert. Werden Semaphore als Mutexe verwendet, ist die Prioritätsvererbung ebenfalls verfügbar. Ein Task kann immer nur an einem Mutex warten, da die Implementierung der Mutexe relativ simpel ist. Wenn ein Mutex von einem niederprioren Task losgelassen wird, kriegt er automatisch seine ursprüngliche Priorität zurück. Die zur Prioritätsvererbung gehörigen Code-Teile werden über Präprozessormacros eingebunden. Sollten also keine Mutexe verwendet werden, sollte das Macro auf jeden Fall auf undefiniert bleiben.

	
	
\subsection{Task Switching}
Unter Task Switching versteht man die Zeit, die der Scheduler braucht, um von einem Task zu einem anderen zu wechseln. Dieser Wechsel wird nach der Scheduling-Strategie des Schedulers vollzogen, d.h. der Task wird nicht etwa durch einen Interrupt oder durch einen höher prioren Task unterbrochen.
\subsubsection{FreeRTOS}
\paragraph{Variante 1}
Es werden zwei Tasks \textit{Task1} und \textit{Task2} erzeugt. Diese Tasks haben einen Workload, der darin besteht, in einer For-Schleife eine Variable hochzuzählen. Nach jedem Inkrementieren der Variable wird ein Context-Switch erzwungen (mit taskYIELD()). Wenn die Variable eine bestimmte Höhre erreicht hat, wird das Experiment beendet. Es wird dabei die Zeit gemessen, die zwischen dem Betreten des ersten Tasks und dem Verlassen des letzten Tasks vergeht. Ein Task-Switch trifft also zwei Mal so häufig auf, wie die Schleife durchgelaufen wird. 
\\\\Von der gemessenen Zeit muss noch der eigentliche Workload abgezogen werden. Dafür werden vor dem Starten der Tasks zwei For-Schleifen durchlaufen mit der gleichen Anzahl an Durchgängen wie in den Tasks.
\paragraph{Variante 2}
Es werden zwei Tasks erzeugt, in denen eine For-Schleife mit der Anzahl der Testdurchläufe ausgeführt wird. In der Schleife befindet sich in der Reihenfolge:
\begin{itemize}
	\item Starte Messung
	\item Erzwinge Task-Switch mit taskYield()
	\item Stoppe Messung
\end{itemize}
Der Vorteil an dieser Methode ist, dass es keinen Overhead gibt. Diese Messung ist also genauer. Von dem Ergebnis muss noch die Messzeit von sechs Zyklen abgezogen werden. Zu beachten ist, dass beide Tasks damit beginnen, die Startzeit der Messung zu speichern. Das führt dazu, dass die erste Startzeit überschrieben wird. Die letzte Startzeit ist dafür ungültig und dard nicht in dem Endergebnis berücksichtigt werden. 

\subsubsection{Linux}
Auch hier ist das Vorgehen wie in Variante 2 von FreeRTOS. Für das taskYield wird die in Linux äquivalente Funktion \textit{sched\_yield} benutzt. Die Randbedingungen für die Linux-Experimente (also nicht nur für dieses hier sondern auch für die folgenden Experimente):
\paragraph{Erzeugen der Tasks}
In Linux muss man beim Erzeugen der Task in folgender Hinsicht aufpassen:\\Da Realzeit-Messungen gemacht werden sollen, wird für diese Thread-Erzeugung das Realzeit-Scheduling von Linux gebraucht. Die Prioritäten-Range liegt über den Prioritäten der normalen Prozesse und geht von 1 bis 99, wobei 99 die höchste Priorität ist. Diese Prioritäten können nur für eine Realzeit-Scheduling-Policy vergeben werden. Hier unterstützt Linux Round-Robin- und FIFO-Scheduling. Um eine faire CPU-Verteilung zu gewährleisten, wird in dem Experiment RR benutzt. Beim Starten des Programms wird deshalb das Scheduling für den Elternprozess verändert und die Priorität auf die maximal mögliche Echtzeit-Priorität gesetzt. Dann werden die eigentlichen Tasks erzeugt, die das Experiment ausführen. Diese haben eine niedrigere Priorität, als der Eltern-Prozess, um ihn nicht vorzeitig zu verdrängen. Wenn beide Tasks erzeugt sind, wird die Priorität es Eltern-Prozesses unter die der Kinderprozesse gesetzt. Diese sind damit am höchsten priorisiert und werden nicht länger durch den Eltern-Prozess blockiert. 

\subsection{Preemption-Zeit}
Die Preemption-Zeit ist die Zeit, die benötigt wird, um einen Task-Switch zu vollziehen, wenn ein niederpriorer Task durch einen Interrupt oder durch einen höherpriorisierten Task oder einen Interrupt unterbrochen wird. Das bedeutet, der Scheduler wird außerhalb des regulären Tick-Interrupts aufgerufen.

\subsubsection{FreeRTOS}
Ein niederpriorer Task verrichtet Arbeit. Dieser Task wird nach einer bestimmten Zeit von einem höher-priorisierten Task unterbrochen. Es gibt zwei Funktionen in FreeRTOS, um einen Delay herbeizuführen: \textit{vTaskDelay} und \textit{vTaskDelayUntil}. \textit{vTaskDelay} wacht nach einer bestimmten Anzahl von Ticks auf. Um die Zeit zu messen, kann die Startzeit in einer Endlosschleife im arbeitenden Task dauerhaft ausgelesen werden. Wenn der Task unterbrochen wird, wurde die aktuellste Zeit vorher gespeichert. Sobald der höher priorisierte Task aufgewacht ist, wird wieder die Zeit gemessen. Das \textit{vTaskDelayUntil} wird nur jeden Tick ausgeführt. 
\\\\Eine andere Möglichekeit, die Preemption Zeit zu messen, ist, dass ein hochpriorer Task sich selbst verabschiedet und hinterher von einem niederprioren Task aufgeweckt wird. Dieses führt direkt zu einem Context-Switch.
\\\\Noch eine Möglichkeit ist es, einen hochpriorisierten Task 1 zu suspendieren und dann einen niederpriorisieren Task 2 laufen zu lassen. Während der Task 2 läuft, wird ein Interrupt ausgelöst, der Task 1 fortsetzt und somit einen Context-Switch erzeugt.
\subsubsection{Linux}

\subsection{Semaphor Shuffle Time}
Die Semaphor Shuffle Time ist die Zeit, die ein Task braucht, um an einem von einem anderen Task genommenem Semaphor aufzuwachen, wenn dieser wieder losgelassen wird. 
\subsubsection{FreeRTOS}
FreeRTOS hat mehrere Semaphorarten: 
\begin{itemize}
	\item Mutex mit Priority inheritance
	\item Binäre Semaphore ohne Priority inheritance
	\item Semaphore, die hochgezählt werden 
\end{itemize}

Mutexe und Counting Semaphores lösen beim freigeben des Semaphores einen \textit{portYield} aus, wodurch es zu einer Verzögerung kommt. 

\paragraph{Mutex}
Ein Mutex wird verwendet, um Mehrfachzugriffe auf Resourcen zu vermeiden. Mutexe können mit Priority Inheritance verwendet werden, dieser Versuch wird aber mit zwei Tasks gleicher Prioritäten durchgeführt. Wenn ein Task einen Mutex genommen hat, muss dieser ihn auch wieder freigeben. Wenn ein anderer Task an diesem Mutex wartet, wird dieser durch die Freigabe wieder aktiviert.
\\\\Der Versuch kann durchgeführt werden, indem ein Task einen Semaphor nimmt und danach ein Context Switch durchgeführt wird. Ein zweiter Task versucht ebenfalls den Semaphor zu nehmen, wird aber blockiert. Als Folge erfolgt wieder ein Task Switch zurück zum ersten Task. Dieser startet eine Zeitmessung, lässt den Semaphor wieder los und veranlasst einen Context Switch. Nun wacht Task 2 auf, da der Semaphor losgelassen wurde. Die Zeitmessung wird beendet. Der Task lässt den Semaphor wieder los und mit einem Context Switch beginnt ein weiterer Durchlauf des Versuchs.
\\\\Im Endergebnis muss berücksichtigt werden, dass ein Mal \textit{xSemaphoreTake}, ein Mal \textit{xSemaphoreGive} und ein Context Switch von der Gesamtzeit abgezogen werden.

\paragraph{Binäre Semaphore}
Binäre Semaphore in FreeRTOS werden ähnlich wie Signale verwendet und diesen eher der Synchronisation als dem Vermeiden von Mehrfachzugriffen. 
\\\\In diesem Versuch wartet der erste Task an dem Semaphor. Ein zweiter Task wird gestartet, initiiert eine Zeitmessung, gibt den Semaphor frei und macht einen Context Switch. Durch das Freigeben ist nun der ander Task wieder aufgewacht und beendet die Zeitmessung. 
\\\\Von dem Endergebnis muss ein Mal \textit{xSemaphoreGive} abgezogen werden. 

\paragraph{Counting Semaphores}
Counting Semaphores sind Semaphore, die ein bestimmtes Kontingent haben und die hochgezählt werden, wenn ein Semaphor freigelassen wird (also eine Ressource verfügbar ist) und wieder runtergezählt werden, wenn ein Semaphor genommen wird (also eine Ressource belegt ist). Ist keine Ressource verfügbar, wird an dem Semaphor gewartet. Ein Task wird wieder aktiv, sobald eine Ressource verfügbar wird. 
\\\\Diese Semaphore können ähnlich wie Mutexe vermessen werden. Da der Mutex zuerst genommen wird, muss der Semaphor mit seiner maximalen Anzahl (im Versuch 1) initialisiert werden. Der Rest des Versuchs ist analog zum Mutex-Versuch. Am Ende müssen auch die gleichen Werte abgezogen werden. 

	
\subsubsection{Linux}

\subsection{Deadlock breaking time}
Diese Zeit ist die Zeit, die benötigt wird, um einen Deadlock durch Prioritätsinversion wieder aufzulösen. Gäbe es diese nicht, wäre folgende Situation ein Deadlock:
\\\\Task 1 hat die niedrigste Priorität und nimmt sich Mutex M. Bevor M freigegeben wird, wird Task 1 durch Task 2 mit einer höheren Priorität unterbrochen. Task 2 wiederum wird durch Task 3 unterbrochen, was die höchste Priorität hat. Task 3 greift nach Mutex M und wird blockiert. Ohne Prioritätsinversion würde Task 2 weiterlaufen und Task 1 für immer unterbrechen, sodass die für Task 3 benötigte Ressource nie freigegeben wird. Prioritätsinversion sorgt dafür, dass Task 1 vorübergehend die Priorität von Task 3 bekommt, nicht mehr von Task 2 blockiert wird, und somit den Mutex wieder freigeben kann. Somit kann der Task mit der höchsten Priorität, Task 3, seine Arbeit beenden.
\\\\Für die Messung ist es irrelevant, ob Task 1 zwischen der Prioritätsinversion und dem Zurückgeben des Mutex noch Arbeit verrichtet. Um die Messung so genau wie möglich zu halten, wird der Mutex direkt zurückgegeben.

\subsubsection{FreeRTOS}

\subsubsection{Linux}

\subsection{Message Passing Latency}
Die Message Passing Latency ist die Zeit, die ein Task braucht, um eine Message von einem anderen Task zu empfangen. In FreeRTOS werden diese Messages über Queues transportiert. Die Zeit messen kann mit folgendem Aufbau:
\\\\Task 1 wird gestartet und wartet an der Queue auf eine Nachricht. Dabei wird der Task blockiert. Dann startet Task 2 und schreibt eine Nachricht in die Queue. Dieses bewirkt, dass der erste Task deblockiert wird. Die Zeitmessung beginnt vor dem Versenden der ersten Nachricht und endet mit dem Empfang. 
	
\subsubsection{FreeRTOS}

\subsubsection{Linux}	
In Linux gibt es mehrere Möglichkeiten, um Queues zu erzeugen. Da die POSIX message queues nicht von dem System unterstützt werden, wird die die System V message queue API benutzt.
	
\section{Speicherzugriffe}
	\begin{enumerate}		
		\item Speicherplatzverbrauch des gesamten Systems
		\item MPU-Unterstützung
		\item In welchem Rahmen sind dynamische Speicherzugriffe möglich?
		\item Ggf. Zeitverbrauch bei Speicherallokation/-fragmentierung
			\begin{enumerate}		
				\item Allokation von z.B. 1000 Paketen und Messen der Zeit
				\item Vergleich von Context Switch mit Speicher Allokation und ohne (?)
				\item Vergleich von verschiedenen Methoden der Speicherallokation $ \rightarrow $ Was ist der Worst Case, der passieren kann?
			\end{enumerate}
	\end{enumerate}
	
\subsection{FreeRTOS}
Es gibt immer eine Mindestfrakturgröße. Außerdem sind die Funktionen Thread-Save, d.h. können durch keinen anderen Task unterbrochen werden. Ausnahmen davon bildet je nach Implementierung Fall 3.

\subsubsection{Heap\_1.c}
Blöcke werden allokiert, wenn genug Speicher da ist und nie wieder freigegeben.

\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|p{11cm}|}
			\hline
			 Zugriffszeit & Konstant \\	
			\hline
			 Worst Case & Nicht mehr genügend Speicher vorhanden  \\
			\hline
			 Schlussfolgerung & Schnell, aber vorher überlegen, ob der Speicher für die Lebensdauer der Anwendung reicht. \\
			\hline
			 Testfall & Einfaches Allozieren, da kein Rechenaufwand durch Freigaben notwendig. \\
			\hline	
		\end{tabular}
		\label{theap1}
\end{table*}


\subsubsection{Heap\_2.c}
Es gibt eine minimale Blockgröße. Es gibt eine Liste, in der die Blöcke nach Größe sortiert sind. Es wird immer der nächst größte Block alloziert $ \rightarrow $Iteration durch Liste. Kein Verschmelzen von Blocks bei Freigabe. Zu große Blocks werden aufgeteilt. Der neu entstandene Block wird wieder in die Liste einsortiert. Nur sinnvoll, wenn der allozierte Speicher immer in etwa die gleiche Größe hat. 

\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|p{11cm}|}
			\hline
			 Zugriffszeit & Am Anfang konstant, weil nur ein Block. Sobald die Liste mehrere Elemente besitzt, ist die Zugriffszeit linear abhängig von der Länge der Liste. \\	
			\hline
			 Worst Case & Nicht mehr genügend Speicher vorhanden oder es ist Speicher vorhanden, aber nicht mehr an einem Stück oder es gibt sehr viele kleine Segmente in der Liste und nur ein größeres ganz hinten  \\
			\hline
			 Schlussfolgerung & Durch Freigaben langsamer als in Fall eins. Nicht sinnvoll, wenn allozierte Blockgröße variiert.\\
			\hline
			 Testfall &  Allozieren von möglichst vielen minimal großen Blöcken und einem, der die doppelte Größe hat. Alle wieder freigeben $ \rightarrow $ Lange Liste mit vielen Einträgen $ \rightarrow $ Nochmal den größeren Block allozieren. Die Zeit für die Längste Freigabe kann auch gemessen werden. \\
			\hline	
		\end{tabular}
		\label{theap2}
\end{table*}

\subsubsection{Heap\_3.c}
Maskierte malloc und free Aufrufe des jeweiligen Compilers.

\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|p{11cm}|}
			\hline
			 Zugriffszeit &  \\	
			\hline
			 Worst Case &  \\
			\hline
			 Schlussfolgerung & \\
			\hline
			 Testfall &  \\
			\hline	
		\end{tabular}
		\label{theap3}
\end{table*}

\subsubsection{Heap\_4.c}
Liste mit Blockzeigern und Blockgröße. Liste wird durchsucht, bis ein passendes Element gefunden wird. Bei Freigabe werden nebeneinander liegende Blöcke wieder zusammengeführt.

\begin{table*}[htb]
	\centering
		\begin{tabular}{|l|p{11cm}|}
			\hline
			 Zugriffszeit & Wie in Fall zwei, aber insgesamt schneller, da Blöcke bei der Freigabe wieder zusammengeführt werden und insgesamt tendenziell weniger Blöcke durchiteriert werden müssen.\\	
			\hline
			 Worst Case & Wie in Fall zwei\\
			\hline
			 Schlussfolgerung & Flexibelste Alternative, Freigabe ist geringfügig langsamer als in Fall zwei, weil Blöcke noch zusammengeführt werden.\\
			\hline
			 Testfall &  Allozieren wie in Fall zwei. Freigabe von jedem zweiten Block, sodass Speicher segmentiert bleibt. Dann nochmal den hintersten Block allozieren. \\
			\hline	
		\end{tabular}
		\label{theap4}
\end{table*}

\subsection{Verifizierung}
\begin{itemize}
	\item Unter welchen Voraussetzungen ist eine Verifizierung möglich?
	\item Verifizierung bei einem ganz bestimmten Szenario
\end{itemize}

\subsection{Multiprozessorunterstützung}