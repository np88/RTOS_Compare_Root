\chapter{Measurements}\label{ch_measurements}
This chapter describes the measurements performed for benchmarking the \acp{OS} FreeRTOS and LinuxRT.
First, the conditions under which the experiments have taken place are described.
This includes hardware platform, time measuring technique and setup of the operation systems under test.
After that, every measurement is described in detail beginning with the boot time.
The results will be discussed in a separate chapter.

\section{Test Environment and tools}
This section contains a description of the test environment - the hardware platform, the development tools and the configuration of the used \acp{OS}.
\subsection{Hardware Platform}
The underlying hardware platform is a Xilinx ZC702 Evaluation Board \cite{xilinx:zc702_ev_board} with an Zynq-7000 XC7Z020 \cite{xilinx:zynq7000}.
The XC7Z020 chip integrates a \ac{PS} and a \ac{PL} on a single die.
Two ARM Cortex-A9 MPCore application processors running at 666 MHz are located in the \ac{PS}.
Both cores inherit a 32 kB instruction and a 32 KB data level 1 cache.
Moreover, they share a 512 kB level 2 cache and ?? kB SRAM.
The \ac{PL} is an Artix-7 from the Xilinx's 7 series \ac{FPGA} technology and can be used to implement custom hardware designs.
Further, the evaluation board provides 128 Mb of \ac{QSPI} flash memory.
\par
The \ac{PS} uses so called \ac{MIO} pins to connect to peripheral devices to the processor.
Those pins can be configured to connect either \acp{GPIO}, Ethernet, \ac{SPI} and others.
Moreover, the \ac{EMIO} pins can be used to extend the number of connected peripherals.
\par
When the Zynq platform is used in future projects, most likely both \ac{PS} and \ac{PL} will be utilized.
This chip is especially useful to combine a powerful \ac{CPU} and own hardware designs.
Therefore, an interrupt generator was implemented in the \ac{PL} part to utilize the \ac{PL}.
It is connected as \ac{GPIO} device.
\par
\subsection{Development Tools}
The Xilinx Design Suite version 14.4 was used to create the test environment and the tests.
The \ac{FPGA} part was designed and synthesized in Xilinx \ac{ISE}.
The software was developed using Xilinx \ac{SDK}.

\subsection{\ac{OS} Configuration}
The operation systems under test are FreeRTOS and LinuxRT.
Both need to be configured such that the implemented interrupt generator can be utilized.

\subsubsection{FreeRTOS}
As already mentioned, interrupt management does not effect the FreeRTOS kernel.
Therefore, the \ac{OS} itself does not need to be configured.
Still, the hardware needs to be initialized appropriately on system startup (refer to \ref{ss_interrupts_in_freertos}) and an \ac{ISR} needs to be implemented. 
Moreover, FreeRTOS needs an \ac{FSBL} which can be created by the \ac{SDK}.
The \ac{FSBL} and the FreeRTOS application must be combined to a boot image.
The FreeRTOS version in this project based on version 7.0.2 and is a special port for Xilinx \ac{SDK} 14.4.


\subsubsection{LinuxRT}\label{sss_linuxrt}
Xilinx provides own distributions of embedded Linux which can be compiled for many different platforms. 
The starting point in this thesis is the Xilinx Linux version 3.6 with the corresponding RT patch.
This was the most recent version available at the beginning of this work.

\paragraph{Overview Linux Development process}
Setting up LinuxRT is composed of many steps compared to FreeRTOS:
\begin{enumerate}
	\item Configuring and building U-Boot (\ac{SSBL})
	\item Configuring and building Linux-Image (wrapped with U-Boot header):
		\begin{enumerate}
			\item Build driver for the new hardware device
			\item Configure kernel
			\item Apply RT Patch
		\end{enumerate}
	\item Creating a device tree blob\footnote{A device tree is a data structure which describes the underlying hardware. A device tree is passed to the \ac{OS} at boot time, so it can initialize the hardware dynamically.\cite{device_tree}}
	\item Building a \ac{FSBL}
	\item Creating a boot image from the files produced in the previous steps
	\item Configuring and building a file system
\end{enumerate}

[Picture of Linux Design Flow]
\paragraph{Configuring Linux}
The goal of the configuration is to create a minimal version of Linux.
This means to eliminate kernel tools (e.g. tracing tools) and unnecessary drivers from the system. 
Driver initialization has also a negative impact on the boot time.
Moreover, drivers can cause a larger memory print of the system and delays during runtime. 
As the detailed process of driver elimination is not of interest at this point, only the most important steps will be pointed out:\\
The \ac{OS} under test is based on a real system which will be used by Siemens in later projects.
Therefore, only the drivers needed in this system will be kept in the kernel.
This does not necessarily have an impact on the implemented test application:
The most important drivers are \ac{SPI}, Ethernet and \ac{UART} driver.
All other drivers for \ac{USB} devices, blue ray or CD player, and more will be removed.
Further, there are options which allow kernel tracing (e.g. ftrace) which will be disabled when running the experiments.  
The final configuration reduces the kernel size from 2,8[?] MB to 1,9 MB. 

\subsection{Time measurement}
The time measurement should be as accurate as possible.
High measuring overheads could falsify the results, so software timers should be avoided. 
One elegant way is to access the \ac{CPU} cycle counter register of the ARM Cortex-A9 processor using inline-assembly code [Reference to the ARM Cortex A9 Architecture Manual].
This code is compiled into four assembly instructions when copying the register value to an array.
The advantages of this method is not only low overhead but also the \ac{OS} independence of the assembly code.
Hence this method can be used for both FreeRTOS and LinuxRT.
However, the \ac{CPU} has to keep running at the same frequency during the whole measurement process, otherwise the results will be wrong.

\subsection{Application Design}
This section provides a short overview over the general application design and execution.
To prevent interruption by other tasks, the real-time tasks must have the highest priority in the system.
Usually, 100000 measurements are collected per operation system per test.
If this is not valid for a special case, it will be mentioned in the benchmark description.
 
\subsubsection{FreeRTOS}
Generally, there are no other tasks in FreeRTOS than the ones created for the test.
Consequently, the real-time tasks have the highest priority in the system anyway.

\subsubsection{LinuxRT}
In Linux, multi-tasking applications are realized using pthreads.
The development of an application should be kept as simple as possible, therefore all applications will be implemented in user space.
All real-time thread in Linux get the highest possible priority using the preemptive \ac{RR} \ac{RT} scheduling class.
As the parent process and its child threads are scheduled individually, the parent has to have a higher priority than the children while creating the child threads.
If not, it will be interrupted by the first thread with higher priority and hence cannot invoke the other ones.
When all threads have been started, the priority of the parent is simply decreased below the one of the lowest real-time thread.
 
\section{Boot time}\label{s_boot_time}
The boot time is the time from powering on the hardware until the first (user) task is run. 
On the ZC702 Evaluation platform, this time can be measured by utilizing user \acp{LED} on the board.  
The default state of those \acp{LED} is on.
By turning them off in the first executed task, the exact power up time of the system can be measured.  
As already mentioned (see \ref{ss_boot_time}), the boot time depends on the complexity of the operation system and the hardware. 
Further, the evaluation board provides three different ways of booting an \ac{OS}:
\begin{enumerate}
	\item Via \ac{JTag}
	\item From \ac{SD} card
	\item From \ac{QSPI} flash
\end{enumerate}
The boot from \ac{QSPI} flash is the fastest and the one most likely used in later projects.
Hence it will be used in the following experiments as well.
The following flow graph illustrates the differences between the boot process in FreeRTOS and LinuxRT.

[Picture Comparison Boot Flow Linux and FreeRTOS] 

\subsection{Booting in FreeRTOS}
The booting process is already described in the previous chapter (see \ref{ss_booting_in_freertos}).
Based on the formula derived in section \ref{ss_boot_time}, the FreeRTOS boot time can be quantified as follows:
\par
$t_{boot}^{free} = t_{rom} + t_{fsbl} +  t_{fpga} + t_{osload} $
\par
Whereas there is no time optimization needed for FreeRTOS because the complete boot takes only 1,1 seconds, there are multiple optimization possibilities for LinuxRT. 

\subsection{Booting in Linux}
The booting process in Linux is shown in figure \ref{}, it can be quantified as follows:
\par
$t_{boot}^{Linux} = t_{rom} + t_{fsbl} +  t_{fpga} + t_{osload} +  t_{ssbl} + t_{tree} + t_{filesys} + t_{boot}$
\par
There are three bottlenecks in this process:
\begin{itemize}
	\item Loading the Image from \ac{QSPI} to \ac{RAM}
	\item Loading the file system from \ac{QSPI} NOR flash to \ac{RAM}
	\item Configuring the \ac{FPGA}
\end{itemize}

The default Linux configuration provided by Xilinx does not include an \ac{FPGA} bitstream and is run from \ac{SD} card.
This takes 15 seconds in total.
The transfer of the Linux image and the file system takes about 12 seconds. 
Besides, by adding a bitstream (copying it to the \ac{SD} card), this time increases by another 15 seconds.   
\par
The very first step to decrease this time has already been done by compressing the Linux kernel (see \ref{sss_linuxrt}).
The next step is to move the bitstream inside of the \ac{RAM} image. 
In Linux, the \ac{PL} can be configures from the \ac{OS} by simply writing the bitstream to the file corresponding to the \ac{FPGA}. 
This decreases the \ac{FPGA} configuration to a range of milliseconds. 
Finally, the last step is to change the default file system to a file system that does not need to be loaded by the \ac{SSBL}.
A good option is to create a \textit{\ac{UBIFS}} flash file system \cite{ubifs}.
\ac{UBIFS} is based on \ac{UBI} which itself works on \ac{MTD} devices \cite{mtd}.
\ac{MTD} is an abstraction layer for raw flash devices (no block devices!) in Linux.
On top of that, the volume management system \ac{UBI} takes care of mapping logical erase blocks to physical ones.  
On one hand, \ac{UBIFS} has the advantage of fast mounting speed which is not dependent on the flash size.
On the other hand, \ac{UBI} initialization depends on the flash size, so the partition size should be kept as low as possible.
In the current configuration the initialization of the flash image takes about 3 seconds.
Nonetheless, an experimental new \ac{UBI} feature called \textit{fastmap} is available from Linux kernel 3.7, which allows the attachment of a \ac{UBI} device in almost constant time \cite{ubi}.
One more advantage is, that \ac{UBIFS} allows to store changes in the file system.
This is not possible when using a ram disk image.
\par
\ac{UBIFS} needs to be enabled in the kernel. 
The initialization is completely removed from the boot loader, it is done on kernel start-up.


\section{Interrupt Latency}
Interrupt latency is defined as the time which passes from triggering an interrupt until the first instruction of the interrupt service routine. 
For this  experiment, the assumption is made that devices in real application are often connected as \ac{GPIO}.
Therefore, the described interrupt generator will be utilized.
It has a 5ms period of which the positive pulse is 5 us and the remaining time is negative.
The interrupt latency measured here does not correspond to the time the \ac{CPU} is actually handling the interrupt (see figure [...]).
The time in this experiment will be measured using an oscilloscope, not the \ac{CPU} clock cycle counter.
The detailed execution flow is described in the following:
\begin{enumerate}
	\item Start of measurement: Interrupt is generated by \ac{FPGA}.
	\item Interrupt is handled by \ac{GIC}.
	\item \ac{CPU} executes \ac{ISR} (depending on implementation)
	\item Stop of measurement: \ac{LED} is turned on by the \ac{ISR}
\end{enumerate}
The only difference between the two operation systems is step number three. 
To calculate the exact interrupt latency, the time it takes to turn on the \ac{LED} has to be subtracted.
Consequently, this requires a further experiment:\\
The \ac{LED} will be toggled in a for-loop.
As the hardware needs some time to proceed the request, another for-loop is executed inside.
The time for the for-loop can be measured individually and be subtracted from the execution time.

[Picture interrupt flow]

\subsection{Interrupt Measurement FreeRTOS}
As described before (see \ref{ss_interrupts_in_freertos}), executing the \ac{ISR} is absolutely independent from the \ac{OS}.
There are only two interrupts which need to be considered here: The timer interrupt (highest priority) and the \ac{GPIO} interrupt.
Consequently, the execution time is expected to be constant except in the cases where the \ac{GPIO} interrupt is preempted by the timer tick.

\subsection{Interrupt Measurement LinuxRT}
There are more interrupts involved (e.g. Ethernet and \ac{UART}) in the LinuxRT system than in FreeRTOS.
Dependent on the application, there is the possibility to implement the \ac{ISR} as a threaded interrupt (default in RT) or to execute the handler in interrupt context (see \ref{ss_threaded_interrupts}).
Both alternatives will be implemented.
It is expected that the non-threaded handling is more predictive.


\section{Task Switching Time} 
The task switching time is the time it takes for a task to start after an other task has finished or yielded the processor.
It is important to point out, that the task switching time is not preemption time because the processor is given away voluntarily.
Both \acp{OS} implement a yield function which allows the calling task to invoke the scheduler.
It can be used in the experiment.
\par
Two tasks of the same priority are set up, their composition is the same. 
Both perform a for-loop with the number of executions.
The first instruction in the loop is the recording of the start time.
Then a yield is performed what causes a context switch. 
The first instruction of the newly scheduled task is the recording of the end time.
Obviously, the first start time is recorded twice because the two tasks are equal.
Hence, the start time of the last test run is not valid.
When the for-loop is executed 50000 times in each task, 100000 context switches are performed in total.
An advantage of this method is that all data is collected without any overhead which has to be subtracted afterwards.

[Picture Execution Flow]
 
\section{Preemption Time}
The preemption time is the time which it takes to interrupt a task and schedule another task of higher priority.
This can happen when a task with a high priority is woken up by an other task or \ac{ISR} triggering a signal or releasing a mutex.
In this experiment, there is one low priority task A and a high priority task B.
B starts first and blocks on a signal event.
A is running in a permanent loop and reading the current \ac{CPU} cycle counter register.
At some point in time, an \ac{IRQ} occurs and the task is interrupted.
From the \ac{ISR}, the signal is triggered to wake up B, consequently, a context switch happens.
When B is activated, it measures the ending time and yields.
This reactivates A and the test starts over.
[diagram]

\section{Semaphore Shuffle Time}
The semaphore shuffle time defined by Kar is used to measure the overhead caused by using semaphores to ensure mutual exclusion.
More precisely, it is the time between a task B being blocked on a semaphore which is taken by task A and and the unblock when the semaphore is released.  
Based on the tests, mutexes are used to implement mutual exclusion.
Besides, it is possible, that a task protects a critical section by mutexes, but is not interrupted by another task.
This case has to be considered as well.
Further, signaling is a widely used feature to synchronize events, therefore it has to be included in the benchmark as well.  
Consequently, the semaphore shuffling results in three different tests.

\paragraph{Semaphore Shuffling Time by Kar}
In the first benchmark, two task are passing a mutex from one to the other.
Task A starts by taking the the mutex and yields.
Then the time measurement is started, task B tries to take the same mutex and is blocked.
Now A is rescheduled, releases the semaphore and yields again.
Task B is unblocked, the time is stopped and B releases the semaphore.
By yielding the processor, task A is scheduled and the test starts over.
\par 
[diagram]

\paragraph{Semaphore in a single Task}
This benchmark is very easy to implement.
A task requires a semaphore and releases it immediately after.
The time it takes to perform these executions is measured.
Then the test starts over.
\par 
[diagram]

\paragraph{Event Signaling}
This benchmark measures the time it takes a task to wake up on a signal event.
Task A blocks on a signal.
After that, task B is scheduled, triggers the signal and yields the processor. 
The trigger causes task A to wake up.
The starting point of the measurement is before task B triggers the signal and the ending point after the awakaning of A.
This sequence is repeated.
\par 
[diagram] 

\section{Message Passing Time}
The message passing time is the time which is takes a task to receive a message sent by another task.
Two tasks with equal priorities can be used for the test.
The first task tries to read from a task queue, it is blocked until a message arrives.
Then the second task is scheduled, sends a message via the message queue and yields immediately. [what kind of message?] 
Consequently, the first task is unblocked and retrieves the message sent.
Now that the queue is empty, the test run can be repeated.
The starting time is measured before a message is sent via the queue by the second task, the ending time after the first task receives the message.
[Picture with execution flow]

\section{Deadlock Breaking Time}
The deadlock breaking time is the time which it takes for the \ac{OS} to resolve priority inversion caused by low priority tasks holding a resource needed by a high-priority task. 
The experiment is set up similar to the given example for priority inheritance (for details refer to \ref{ss_priority_inheritance}) and repeated 100000 times: 
Task A with the lowest priority starts executing.
A takes a mutex and starts a second task B with higher priority than itself.
B preempts the first task and starts task C with the highest priority. 
C tries to take the same mutex as the first task and is blocked.
Consequently, the priority of the A is raised, it can finish execution (in the test scenario there is no work to be done), release the mutex and unblock C.
The starting time is measured before C takes the mutex, the ending time after C returns from the Blocked state.
To repeat the test, the original state of the tasks has to be restored.
Therefore, C cancels task B, so it will not preempt A and suspends itself.
Now A will be scheduled and the test can run again.

[Picture with execution flow]
