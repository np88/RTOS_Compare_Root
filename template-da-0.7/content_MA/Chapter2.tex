\chapter{Measurements}\label{ch_measurements}
This chapter describes the measurements performed for benchmarking the \acp{OS} FreeRTOS and LinuxRT.
First, the conditions under which the experiments have taken place are described.
This includes hardware platform, time measuring technique and setup of the operation systems under test.
After that, every measurement is described in detail beginning with the boot time.
The results will be discussed in a separate chapter.

\section{Test Environment and tools}
This section contains a description of the test environment - the hardware platform, the development tools and the configuration of the used \acp{OS}.
\subsection{Hardware Platform}
The underlying hardware platform is a Xilinx ZC702 Evaluation Board \cite{xilinx:zc702_ev_board} with an Zynq-7000 XC7Z020 \cite{xilinx:zynq7000} (see figure \ref{fig_zynq_block_diagram}).
The XC7Z020 chip integrates a \ac{PS} and a \ac{PL} on a single die.
Two ARM Cortex-A9 MPCore application processors running at 666 MHz are located in the \ac{PS}.
Both cores inherit a 32 kB instruction and a 32 KB data level 1 cache.
Moreover, they share a 512 kB level 2 cache and 256 kB SRAM on-chip memory.
The platform also provides 1 GB of DDR3 which is comprised of four 256 Mb x 8 SDRAM that are 32 bits wide. 
The \ac{PL} is an Artix-7 from the Xilinx's 7 series \ac{FPGA} technology and can be used to implement custom hardware designs.
Further, the evaluation board provides 128 Mb of \ac{QSPI} flash memory.

\begin{figure}[htb]
		\begin{center}
			\includegraphics[scale=0.9]{inputs/pictures_ch2/zynq_block_diagram}
			\caption[Zynq-7000 AP SoC Block Diagram]{Zynq-7000 AP SoC Block Diagram \cite{xilinx:zc702_ev_board}} \label{fig_zynq_block_diagram}
		\end{center}
\end{figure} 

The \ac{PS} uses so called \ac{MIO} pins to connect to peripheral devices to the processor.
Those pins can be configured to connect either \acp{GPIO}, Ethernet, \ac{SPI} and others.
Moreover, the \ac{EMIO} pins can be used to increase the number of connected peripherals.
\par
The Zynq platform will be used in projects where most likely both \ac{PS} and \ac{PL} will be utilized.
This chip is especially useful to combine a powerful \ac{CPU} and own hardware designs.
To make use of the \ac{PL}, a pulse generator \ac{FPGA} module was implemented.
It is connected as \ac{GPIO} device.
The pulse generator is used to trigger interrupts which are then handles by the \ac{GIC} of the \ac{APU} (see figure \ref{fig_pulse_generator}). 

\begin{figure}[htb]
		\begin{center}
			\includegraphics[scale=0.71]{inputs/pictures_ch2/pulse_generator}
			\caption{Integration of pulse generator} \label{fig_pulse_generator}
		\end{center}
\end{figure} 

\subsection{Development Tools}
The Xilinx Design Suite version 14.4 was used to create the test environment and the tests.
The \ac{FPGA} part was designed and synthesized in Xilinx \ac{ISE}.
The software was developed using Xilinx \ac{SDK}.
Both systems run on one \ac{CPU} because there is no official FreeRTOS version available which supports multiple processors.
As both systems shall be as equal as possible, Linux was also configured to run on one core only.

\subsection{\ac{OS} Configuration}
The operation systems under test are FreeRTOS and LinuxRT.
Both need to be configured such that the implemented interrupt generator can be utilized.

\subsubsection{FreeRTOS}
As already mentioned, interrupt management does not effect the FreeRTOS kernel.
Therefore, the \ac{OS} itself needs only some basic configuration, e.g. the maximum number of priorities is set to 4 (maximum used in the benchmarking programs).
Still, the hardware needs to be initialized appropriately on system startup (refer to \ref{ss_interrupts_in_freertos}) and an \ac{ISR} needs to be implemented for the pulse generator. 
Moreover, FreeRTOS needs an \ac{FSBL} which can be created by the \ac{SDK}.
The \ac{FSBL}, the FreeRTOS application and the \ac{FPGA} configuration file must be combined to a boot image.
The FreeRTOS version in this project based on version 7.0.2 and is a special port for Xilinx \ac{SDK} 14.4 which was contributed by the FreeRTOS community.

\subsubsection{LinuxRT}\label{sss_linuxrt}
Xilinx provides own distributions of embedded Linux which can be compiled for many different platforms. 
The starting point in this thesis is the Xilinx Linux version 3.6 with the corresponding RT patch.
This was the most recent version available at the beginning of this work.

\paragraph{Overview of the Linux Development process}
Setting up LinuxRT is composed of many steps (see fig. \ref{fig_xilinx_tool_flow}) compared to FreeRTOS:
\begin{enumerate}
	\item Configuring and building U-Boot (\ac{SSBL})
	\item Configuring and building Linux-Image (wrapped with U-Boot header):
		\begin{enumerate}
			\item Build driver for the new hardware device
			\item Configure kernel
			\item Apply RT Patch
		\end{enumerate}
	\item Creating a device tree blob\footnote{A device tree is a data structure which describes the underlying hardware. A device tree is passed to the \ac{OS} at boot time, so it can initialize the hardware dynamically.\cite{device_tree}}
	\item Building an \ac{FSBL}
	\item Configuring and building a file system
	\item Creating a boot image (\textit{zlib} compressed) from the files produced in the previous steps 
\end{enumerate}

\begin{figure}[htb]
		\begin{center}
			\includegraphics[scale=0.9]{inputs/pictures_ch2/xilinx_tool_flow}
			\caption[Development process of a Linux boot image]{Development process of a Linux boot image \cite{xilinx:gt}} \label{fig_xilinx_tool_flow}
		\end{center}
\end{figure} 

\paragraph{Configuring Linux}
The goal of the configuration is to create a minimal version of Linux.
This means to eliminate kernel tools (e.g. tracing tools) and unnecessary drivers from the system. 
Driver initialization has also a negative impact on the boot time.
Moreover, drivers can cause a larger memory print of the system and delays during runtime. 
As the detailed process of driver elimination is not of interest at this point, only the most important steps will be pointed out:\\
The \ac{OS} under test is based on a real system which will be used by Siemens in later projects.
Therefore, only the drivers needed in this system will be kept in the kernel.
This does not necessarily have an impact on the implemented test application:
The most important drivers are \ac{SPI}, Ethernet and \ac{UART} driver.
All other drivers for \ac{USB} devices, blue ray or CD player, and more will be removed.
Further, there are options which allow kernel tracing (e.g. ftrace) which will be disabled when running the experiments.  
The final configuration reduces the kernel size from 2.8 MB to 1.9 MB. 

\subsection{Time measurement}
The time measurement should be as accurate as possible.
High measuring overheads could falsify the results, so software timers should be avoided. 
One elegant way is to access the \ac{CPU} cycle counter register of the ARM Cortex-A9 processor using inline-assembly code \cite{arm:aarmaaae}.
This code is compiled into four assembly instructions [ref to appendix] when copying the register value to an array.
The advantages of this method is not only low overhead but also the \ac{OS} independence of the assembly code.
Hence this method can be used for both FreeRTOS and LinuxRT.
However, the \ac{CPU} has to keep running at the same frequency during the complete test run, otherwise the results will be wrong.

\subsection{Application Design}
This section provides a short overview over the general application design and execution.
To prevent interruption by other tasks, the real-time tasks must have the highest priority in the system.
Usually, 100000 measurements are collected per operation system per test.
If this is not valid for a special case, it will be mentioned in the benchmark description.
The scheduling for both systems is configured to be round robing, hence both are using a preemptive scheduling algorithm.
 
\subsubsection{FreeRTOS}
Generally, there are no other tasks in FreeRTOS than the ones created for the test and the idle task.
Consequently, the real-time tasks have the highest priority in the system anyway.
For all system calls, the FreeRTOS \ac{API} is used. 
%Nonetheless, a FreeRTOS port which includes \ac{MPU} support exists for ARM Cortex-M3 microcontrollers.
%Using this post, tasks can be created either in user or in privileged mode \ref{freertos}.
%This function is not supported by the used port.

\subsubsection{LinuxRT}
In Linux, multi-tasking applications are realized using pthreads.
The development of an application should be kept as simple as possible, therefore all applications will be implemented in user space.
All real-time threads in Linux get the highest possible priority using the preemptive \ac{RR} \ac{RT} scheduling class.
As the parent process and its child threads are scheduled individually, the parent has to have a higher priority than the children while creating the child threads.
If not, it will be interrupted by the first thread with higher priority and hence cannot invoke the other ones.
When all threads have been started, the priority of the parent is simply decreased below the one of the lowest real-time thread.
\par
For thread synchronization, the \ac{POSIX} \ac{API} is used whenever possible as this is the commonly used multi-threading \ac{API} for Linux systems.
It can be utilized for all experiments except for the message passing experiment, because this part of the \ac{API} is not supported by the platform.
Instead, the message queue \ac{API} provided by the \textit{System V} \ac{IPC} is used.
System V (read: \textit{System five}) \cite{tlip:svd} is a Unix \ac{OS} version from 1983 and its \ac{API} is still provided by modern Linux distributions.
\par
Further, the \textit{mlockall} instruction is used to prevent page swapping during the execution of the test application.
Another important factor is not to expose the system to external interrupts from the Ethernet or \ac{UART} devices.
[Daemens]
By considering this, $ t_swap $ and $ t_int $ can be set to zero for the Linux system.
 
\section{Boot time}\label{s_boot_time}
The boot time is the time from powering on the hardware until the first (user) task is run. 
On the ZC702 Evaluation platform, this time can be measured by utilizing user \acp{LED} on the board.  
The default state of those \acp{LED} is on.
By turning them off in the first executed task, the exact power up time of the system can be measured.  
As already mentioned (see \ref{ss_boot_time_general}), the boot time depends on the complexity of the operation system and the hardware. 
Further, the evaluation board provides three different ways of booting an \ac{OS}:
\begin{enumerate}
	\item Via \ac{JTAG}
	\item From \ac{SD} card
	\item From \ac{QSPI} flash
\end{enumerate}
The boot from \ac{QSPI} flash is the fastest and the one most likely preferred in later projects.
Hence it will be used in the following experiments as well.
The flow graphs illustrate the differences between the boot process in FreeRTOS (fig. \ref{fig_boot_freertos}) and LinuxRT (fig. \ref{fig_boot_linux_xilinx} and fig. \ref{fig_boot_linux_custom}).

\subsection{Booting in FreeRTOS}
The booting process is already described in the previous chapter (see \ref{ss_booting_in_freertos}).
Based on the information in section \ref{s_boot_time}, the FreeRTOS boot time can be described as follows:
	\begin{equation}
		t_{boot}^{free} = t_{rom} + t_{fsbl} +  t_{fpga} + t_{osload} \label{eq_t_boot_free} 
	\end{equation}
Whereas there is no time optimization needed for FreeRTOS because the complete boot takes only 1,1 seconds, there are multiple optimization possibilities for LinuxRT. 

\begin{figure}[htb]
		\begin{center}
			\includegraphics[scale=0.7]{inputs/pictures_ch2/boot_freertos}
			\caption[FreeRTOS boot]{FreeRTOS Boot \cite{xilinx:zbff}} \label{fig_boot_freertos}
		\end{center}
\end{figure} 

\subsection{Booting in Linux}\label{ss_booting_in_linux}
The booting process in Linux is shown in figure \ref{fig_boot_linux_xilinx}, it can be described as follows:
\begin{equation}
t_{boot}^{Linux} = t_{rom} + t_{fsbl} +  t_{fpga} + t_{osload} +  t_{ssbl} + t_{tree} + t_{filesys} + t_{boot}
\label{eq_t_boot_linux} 
\end{equation}

\begin{figure}[htb]
	\begin{center}
	\begin{minipage}[bth]{6,8cm}
		\begin{center}
			\includegraphics[scale=0.7]{inputs/pictures_ch2/boot_linux_xilinx}
			\caption[Xilinx Standard Boot]{Xilinx Standard Boot \cite{xilinx:zbff}}
		\end{center}	
	\end{minipage}\label{fig_boot_linux_xilinx}
	\hspace{0,5cm}
	\begin{minipage}[bth]{6,8cm}
		\begin{center}
			\includegraphics[scale=0.7]{inputs/pictures_ch2/boot_linux_custom}
			\caption[Xilinx Standard Boot]{Xilinx Custom Boot \cite{xilinx:zbff}} 
		\end{center}
	\end{minipage}\label{fig_boot_linux_custom}
	\end{center}
\end{figure}

There are three bottlenecks in this process:
\begin{itemize}
	\item Loading the Image from \ac{QSPI}
	\item Loading the file system from \ac{QSPI} NOR flash
	\item Configuring the \ac{FPGA}
\end{itemize}

The default Linux configuration provided by Xilinx does not include an \ac{FPGA} bitstream and is run from \ac{SD} card.
This takes 13 seconds in total.
Whereas $t_{boot}$ is only 3 seconds and $t_{fpga}$ is 0, the other delays sum up to 10 seconds, which is mostly due to the transfer of the Linux image and the file system . 
Besides, by adding a bitstream to the boot image, this time increases by another $ t_{fpga} $ which depends on the size of the bit stream.   
\par
The very first step to decrease this time has already been done by compressing the Linux kernel (see \ref{sss_linuxrt}).
The next step is to move the bitstream inside of the boot image.
This has to be done not only to speed up the booting process but also because of the limited \ac{QSPI} size of 128 Mb.
By moving the bitstream into the boot image, it is compressed as well. 
In Linux, the \ac{PL} can be configured from the \ac{OS} by simply writing the bitstream to the device file corresponding to the \ac{FPGA}. 
This decreases $t_{fpga}$ to a range of milliseconds. 
Finally, the last step is to change the default file system to a file system that does not need to be loaded by the \ac{SSBL}.
A good option is to create a \textit{\ac{UBIFS}} flash file system \cite{ubifs}.
\ac{UBIFS} is based on \textit{\ac{UBI}} which itself works on \textit{\acp{MTD}} \cite{mtd}.
\ac{MTD} is an abstraction layer for raw flash devices (no block devices!) in Linux.
On top of that, the volume management system \ac{UBI} takes care of mapping logical erase blocks to physical ones.  
On one hand, \ac{UBIFS} has the advantage of fast mounting speed which is not dependent on the flash size.
On the other hand, \ac{UBI} initialization depends on the flash size, so the partition size should be kept as low as possible.
In the current configuration the initialization time of the file system $t_{filesys}$ takes 3 seconds and is part of $t_{osload}$.
Nonetheless, an experimental new \ac{UBI} feature called \textit{fastmap} is available from Linux kernel 3.7, which allows the attachment of a \ac{UBI} device in almost constant time \cite{ubi}.
Another advantage is, that \ac{UBIFS} allows changes to the file system to be stored at power-off.
This is not possible when using a standard ram disk image.
\par
The file system initialization is completely removed from the boot loader, it is done on kernel start-up.
A disadvantage of this method is that \ac{UBIFS} needs a reconfiguration of the kernel.
The new boot process is shown in figure \ref{fig_boot_linux_custom}. 

\section{Task Switching Time} 
The task switching time is the time it takes for a task to start after an other task has finished or yielded the processor.
It is important to point out, that the task switching time is not preemption time because the processor is given away voluntarily.
The task switching latency shows the minimum overhead in a system with at least two tasks for the maximum priority. 
This value is also valid for systems with more real-time tasks of the same priority which use the round robin scheduling algorithm.
For task switching between different priority levels, the scheduling is dependent on the number of priority level.
In such cases, the complete list has to be searched from the beginning.
Therefore, the task switching time is slightly higher than for task switching within the same priority level.
Both \acp{OS} implement a yield function which allows the calling task to invoke the scheduler.
It can be used in the experiment.
\par
Two tasks of the same priority are set up, their composition is the same. 
Both perform a for-loop with the number of executions.
The first instruction in the loop is the recording of the start time.
Then a yield is performed what causes a context switch (see table \ref{tab_task_switching}). 
The first instruction of the newly scheduled task is the recording of the end time.
Obviously, the first start time is recorded twice because the two tasks are equal.
Hence, the start time of the last test run is not valid.
When the for-loop is executed 50000 times in each task, 100000 context switches are performed in total.
An advantage of this method is that all data is collected without any overhead which has to be subtracted afterwards.

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l|l|l|}
			\hline
				& Task 1							& Task 2						\\
				\hline 
			  1 & Start time					& --								\\
			  2 & Yield								& --								\\
			  3 & --									& Stop time					\\ 
			  4 & --									& Start time				\\
			  5 & --									& Yield							\\
			  6 & Stop time						& --								\\ 			  
			\hline
		\end{tabular}
	\caption{Task Switching Algorithm}
	\label{tab_task_switching}
\end{table}
 
\section{Semaphore Shuffle Time}
The semaphore shuffle time defined by Kar is used to measure the overhead caused by using semaphores to ensure mutual exclusion.
More precisely, it is the time between a task B being blocked on a semaphore which is taken by task A and then unblocked when the semaphore is released.  
Besides, it is possible, that a task protects a critical section by mutexes, but is not interrupted by another task.
This case has to be considered as well.
Based on the above test cases, not counting semaphores but mutexes are used to implement mutual exclusion.
\par
Furthermore, signaling is a widely used feature to synchronize events, therefore it has to be included in the benchmark as well.  
Consequently, the semaphore shuffling results in three different tests.
The synchronization tests are important as they can influence the design of an application.
For time critical applications, it is preferable to use the fastest methods avaiable.

\paragraph{Semaphore Shuffling Time by Kar}
In the first benchmark, two tasks are passing a mutex from one to the other.
Task A starts by taking the the mutex and yields.
Then the time measurement is started, task B tries to take the same mutex and is blocked.
Now A is rescheduled, releases the semaphore and yields again.
Task B is unblocked, the time is stopped and B releases the semaphore.
By yielding the processor, task A is scheduled and the test starts over (compare table \ref{tab_sem_kar}).

\begin{table}[htbp]
\begin{minipage}[t]{0.45\textwidth}
	\begin{center}
		\begin{tabular}{|l|l|l|}
			\hline
				& Task 1							& Task 2						\\
				\hline 
			  1 & Take mutex					& --								\\
			  2 & Yield								& --								\\
			  3 & --									& Start time				\\ 
			  4 & --									& Block at mutex		\\
			  5 & Release mutex				& --								\\
			  6 & Yield								& --								\\
			  7 & --									& Stop time					\\ 
			  8 & --									& Release mutex			\\
			  9 & --									& Yield							\\			  
			\hline
		\end{tabular}
	\caption{Semaphore Shuffling Time proposed by Kar}
	\label{tab_sem_kar}
	\end{center}
	\end{minipage} \hfill 
	\begin{minipage}[t]{0.45\textwidth}
	\begin{center}
		\begin{tabular}{|l|l|}
			\hline
				& Task 							\\
				\hline 
				1 & Start time				\\ 
			  2 & Take mutex				\\
				3 & Release mutex			\\						 
			  4 & Stop time					\\ 			  
			\hline
		\end{tabular}
	\caption{Semaphore in one single task}
	\label{tab_sem_single}
	\end{center}
	\end{minipage}
\end{table}

\paragraph{Semaphore in a single Task}
This benchmark is very easy to implement.
A task requires a semaphore and releases it immediately after.
The time it takes to perform these executions is measured.
Then the test starts over (compare table \ref{tab_sem_single}).

\paragraph{Event Signaling}
This benchmark measures the time it takes a task to wake up on a signal event.
Task A blocks on a signal.
After that, task B is scheduled, triggers the signal and yields the processor. 
The trigger causes task A to wake up.
The starting point of the measurement is before task B triggers the signal and the ending point after the waking up of A.
This sequence is repeated (see table \ref{tab_binary_semaphore}).

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l|l|l|}
			\hline
				& Task 1 							 	 & Task 2 										\\
				\hline 
				1 & --									   & Block on signal						\\
			  2 & Start time		 				 & --													\\
			  3 & Trigger signal				 & --													\\
			  4 & Yield								 	 & --													\\
			  5 & --									   & Wake up										\\
			  6 & --									 	 & Stop time									\\	  
			\hline
		\end{tabular}
	\caption{Algorithm to measure wake up time on binary semaphore}
	\label{tab_binary_semaphore}
\end{table}

\section{Message Passing Time}
The message passing time is the time which is takes a task to receive a message sent by another task.
As message passing is also used for task synchronization, its latency can have an impact on the application design.
Two tasks with equal priorities can be used for the test.
The first task tries to read from a task queue, it is blocked until a message arrives.
Then the second task is scheduled, sends a message via the message queue and yields immediately. 
The messages created have a size of one byte.
Consequently, the first task is unblocked and retrieves the message sent.
Now that the queue is empty, the test run can be repeated.
The starting time is measured before a message is sent via the queue by the second task, the ending time after the first task receives the message (see table \ref{tab_message_passing}).

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l|l|l|l|}
			\hline
				& Task 1 							 & Task 2 								\\
				\hline 
				1 & --									 & Block on message queue	\\
			  2 & Start time		 			 & --											\\
			  3 & Put message to queue & --											\\
			  4 & Yield								 & --											\\
			  5 & --									 & Wake up								\\
			  6 & --									 & Stop time							\\	  
			\hline
		\end{tabular}
	\caption{Message passing algorithm}
	\label{tab_message_passing}
\end{table}

\section{Deadlock Breaking Time}
\begin{table}[htbp]
	\centering
		\begin{tabular}{|l|l|l|l|}
			\hline
				& Task 1 (low priority)						& Task 2 (medium priority)	&  Task 3 (high priority)	\\
				\hline 
				1 & --															& Block on event						& --									\\
			  2 & --															& --												& Block on event			\\
			  3 & Take Semaphore 									& --												& --									\\
			  4 & Unblock task  2									& --							 					& --									\\
			  5 & Yield														& --												& --									\\
			  6 & --															&	Unblock task 3						& --									\\
			  7 & --															& Yield											& --									\\
			  8 & --															&	--												& Start time					\\  
			  9 & --															&	--												& Block on Semaphore	\\  	
			  10 & --															& Do work while 3 blocked		& --									\\	
			  11 & Unblocked by priority raise		&	--												& --									\\
			  12 & Give back semaphore						&	--												& --									\\	
			  13 & Yield													& --	  										& --									\\
			  14 & --															&	--												& Stop time						\\
			  15 & --															&	--												& Yield								\\
			\hline
		\end{tabular}
	\caption{Deadlock breaking time}
	\label{tab_deadlock}
\end{table}

The deadlock breaking time is the time which it takes for the \ac{OS} to resolve priority inversion caused by low priority tasks holding a resource needed by a high-priority task (see table \ref{tab_deadlock}). 
If this time is too large on specific systems, the application has to be redesigned by removing the priority inversion. 
The experiment is set up similar to the given example for priority inheritance (for details refer to \ref{ss_priority_inheritance}): 
First, the high priority tasks are blocked on events.
Then task A with the lowest priority starts executing.
A takes a mutex and unblocks the second task B with higher priority than itself.
B preempts the first task and unblocks task C with the highest priority. 
C tries to take the same mutex as the first task and is blocked.
Consequently, the priority of the A is raised, it can finish execution (in the test scenario there is no work to be done), release the mutex and unblock C.
The starting time is measured before C takes the mutex, the ending time after C returns from the Blocked state.
To repeat the test, the original state of the tasks has to be restored.
Therefore, C cancels task B, so it will not preempt A and suspends itself.
Now A will be scheduled and the test can run again.

\section{Interrupt Latency}
Interrupt latency is defined as the time which passes from triggering an interrupt until the first instruction of the interrupt service routine. 
As interrupts are handled differently in the two \acp{OS}, the interrupt latency can be a crucial factor when fast interrupt handling is important.
For this  experiment, the assumption is made that devices in real application are often connected as \acp{GPIO}.
Therefore, the described pulse generator will be utilized.
It has a 5ms period of which the positive pulse is 5 us and the remaining time is negative.
The interrupt latency measured here does not correspond to the time the \ac{CPU} is actually handling the interrupt (see table \ref{tab_interrupt_latency}).
It measures the time from the interrupt being triggered until the output of the \ac{ISR} is visible on the hardware.
The latency in this experiment will be measured using an oscilloscope, not the \ac{CPU} clock cycle counter.
The detailed execution flow is described in the following:

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l|l|l|}
			\hline
				FreeRTOS 																					& Linux							& Time measurement  \\
				\hline 
				\multicolumn{2}{ |c| }
					{Interrupt is generated by \ac{FPGA}}					 	& Start 	\\
			  \multicolumn{2}{ |c| }
			  	{Interrupt is handled by \ac{GIC}}							&					\\
			  \multicolumn{2}{ |c| }
			  	{Interrupt handling by invoking the processor}	&					\\		
			  \multicolumn{2}{ |c| }{ }	& \\	  	
			  \hline
			  																									& Calling do\_IRQ() function											&		\\
			  Calling registered \ac{ISR}												& Checking whether an interrupt handler exists 	& 	\\
			  																									& If yes, perform \ac{ISR}											& 	\\
		 		\hline
		 		\multicolumn{2}{ |c| }{ }	& \\	
		 		\multicolumn{2}{ |c| }{Turn off \ac{LED}}					& Stop	\\
		 		\multicolumn{2}{ |c| }{Return}										&			 	\\	   
			\hline
		\end{tabular}
	\caption{Interrupt latency in FreeRTOS and Linux}
	\label{tab_interrupt_latency}
\end{table}

The only difference between the two operation systems is the \ac{IRQ} handling.
Table \ref{tab_interrupt_latency} shows how much time is spent when the actions are performed in the \ac{ISR} directly. 
Yet, there is often the case that a thread is resumed from within the \ac{ISR}.
This case should be considered as well.
In table \ref{tab_interrupt_latency}, the entry \textit{Continuing user task} has to be added.
The overhead for starting a separate task from an \ac{ISR} can be calculated from these two experiments.  
%To calculate the exact interrupt latency, the time it takes to turn on the \ac{LED} has to be subtracted.
%Consequently, this requires a further experiment:\\
%The \ac{LED} will be toggled in a for-loop.
%As the hardware needs some time to proceed the request, another for-loop is executed inside.
%The time for the for-loop can be measured individually and be subtracted from the execution time.

\subsection{Interrupt Measurement FreeRTOS}
As described before (see \ref{ss_interrupts_in_freertos}), executing the \ac{ISR} is absolutely independent from the \ac{OS}.
There are only two interrupts which need to be considered here: The timer interrupt (highest priority) and the \ac{GPIO} interrupt.
Consequently, the execution time is expected to be constant except in the cases where the \ac{GPIO} interrupt is preempted by the timer tick.
For FreeRTOS, no experiments under load conditions are necessary because the system itself is designed to fit a special application.
It is not designed to be exposed to unexpected conditions. 

\subsection{Interrupt Measurement LinuxRT}
There are more interrupts involved (e.g. Ethernet and \ac{UART}) in the LinuxRT system than in FreeRTOS.
Dependent on the application, there is the possibility to implement the \ac{ISR} as a threaded interrupt (default in RT) or to execute the handler in interrupt context (see \ref{ss_threaded_interrupts}).
Both alternatives will be implemented.
It is expected that the non-threaded handling is faster and more predictive.
As this method of interrupt handling is the standard on the mainline kernel, the mainline kernel will be used to test the non-threaded interrupts.
\par
Interrupt service routines in Linux can only be installed in kernel modules. 
This means that the only way to be notified of an interrupt occurrence is to perform e.g. a read command to the driver of a hardware device and be blocked until the interrupt occurs.
It is always the easier way and therefore preferred by a developer to write a program running in user mode than an own driver.
Further, some programs might need to use the \ac{FPU} what should be avoided in kernel mode (\cite{love:lkd}).
The \ac{FPU} registers have to be manually saved and restored inside the kernel.
Moreover, floating point instructions are complex and decrease the runtime of the particular kernel code. 
\par
Further, it is part of the test to see how stable the Linux RT system works under load conditions.
Load conditions can be either \ac{CPU} bound or ac{I/O} bound. 
\ac{CPU} bound conditions are for example many working threads running concurrently on the system.
This can be simulated running the \textit{Stress} \cite{stress} program.
Stress can run a definite number of working threads on a system that produce for example \ac{CPU} and/or \ac{I/O} load.
To put the system under \ac{I/O} load, the Ethernet connection can be used.
It does not only generate hardware interrupts but also softirqs.
Ethernet is a good choice because it will be most likely used in real applications.
The load can be created by constantly transferring data to the system under test.
This can be done for example by using the \ac{SSH} protocol \cite{tlip:tsspa}.
To prevent the system from being overload by one large file, a small file is sent over and over again. 

\section{Preemption Time}
\begin{table}[htbp]
	\centering
		\begin{tabular}{|l|l|l|l|}
			\hline
				& Task 1 (low priority)& Task 2 (high priority)	& \ac{ISR}				\\
				\hline 
				1 & --									 & Block on event					& --							\\
			  2 & Loop: Start time		 & --											& --							\\
			  3 & --									 & --											& Stop task 1			\\
			  4 & --									 & --							 				& Wake up task 2	\\
			  5 & --									 & Stop time							& --							\\	  
			\hline
		\end{tabular}
	\caption{Preemption from ISR}
	\label{tab_preemption}
\end{table}

The preemption time is the time which it takes to interrupt a task and schedule another task of higher priority.
This can happen when a task with a high priority is woken up by an other task or \ac{ISR} triggering a signal or releasing a mutex.
This experiment is important because it simulates a situation which often happens in real applications.
In this experiment, there is one low priority task A and a high priority task B.
B starts first and blocks on a signal event.
A is running in a permanent loop and reading the current \ac{CPU} cycle counter register.
At some point in time, an \ac{IRQ} from the pulse generator occurs and the task is interrupted.
From the \ac{ISR}, the signal is triggered to wake up B, consequently, a context switch happens.
When B is activated, it measures the ending time and yields.
This reactivates A and the test starts over (compare table \ref{tab_preemption}).
The test is repeated 25000 times.
%\section{Test program}