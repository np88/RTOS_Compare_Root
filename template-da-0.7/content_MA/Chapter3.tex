\chapter{Results}\label{ch_results}
In this chapter, the results from the previously described experiments are presented.
Moreover, the indeterministic aspects of the are further investigated for both operation systems. 
The main observation is - as expected - that LinuxRT is usually slower than FreeRTOS and has a higher jitter. 
This is especially recognizable when the benchmarks become more complex, for example for the deadlock breaking time or for the preemption time.
To create the same initial situation for every benchmark, the Linux operation system was rebooted every time before performing a test.

\section{Boot Time}
This section mainly shows the possible decrease of the boot time in Linux as there was no need to further improve the start up time of FreeRTOS.

\subsection{Boot Time of FreeRTOS}
The boot time of FreeRTOS has been quantified by formula \ref{eq_t_boot_free}.
As mentioned before, there is no need to optimize the FreeRTOS boot time, so the final $ t_{boot}^{free} $ is 1,1 s.

\subsection{Boot Time of Linux}
The boot time in Linux $ t_{boot}^{linux} $ was initially 20 seconds when using the boot configuration with a custom kernel from Xilinx (see \ref{ss_booting_in_linux}). 
With the new boot configuration described in section \ref{ss_booting_in_linux} the boot time could be reduced to 7.5 seconds. 
One main factor is the \ac{FPGA} configuration of the \ac{FSBL} which takes 11 seconds in the original configuration.
The change of the file system has an impact of 2 seconds as well but this is a minor factor as the \ac{UBI} gets slower with increasing memory. 
Extensive use of the system caused for example a command history and further cache data to be stored and checked on start-up.
This leads to an increase of the boot time to 9 seconds when the series of benchmarks was finished.
Yet, it is still possible to load the file system in read-only mode which decreases $ t_{boot}^{linux} $ to 5.7 s.
This can be for example used in a running systems where no writing of data to the file system is necessary.
In configuration mode, the system can be easily booted in read-write mode so that existing applications or configuration files can be updated. 
The result of this experiment is a boot time which was decreased by 55 \% in read-write mode and by 71.5 \% in read-only mode.

\section{Task Switching Results}\label{s_task_switching_results}
The measurements are presented in two different graphs. 
The upper one always shows the number of the occurrence of one latency over time. 
This curve gives a good overview over the statistic and deviation of the measurement.
The lower picture shows the latencies plotted over time. 
This curve shows time dependent events like the timer tick.  
%Further, this presentation of the results is not only valid for the task switching time but for all results presented in the following.

\subsection{Task Switching Results FreeRTOS} \label{ss_task_switching_results_freertos}
\begin{figure}[hbt]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/task_switching_debug_FreeRTOS_start_end}
	\end{center}
	\caption{Task switching result FreeRTOS} \label{fig_tast_switching_result_free}
\end{figure}
As the red color in figure \ref{fig_tast_switching_result_free} shows, the mean value for FreeRTOS task switching is 238 ns. 
The standard deviation for the test set is 3.8 ns (1.6 \%).
The peaks at the time of 0.1 and 0.2 us are showing the active timer tick. 
The probability to capture the timer tick in this experiment is very high because a large density of the test program code is included into the time measurement.
The maximum value measured for the tick is 1.059 us.
When the task switching time is subtracted from this value, the resulting tick overhead is 821 ns.
The reason for the different tick times (1.059 us and 0.752 us) cannot be clearly verified, as no proper tracing tools were available. 
Yet, some assumptions can be made on the behavior of the system. 
In table \ref{tab_task_switching}, the test algorithm is described. 
Depending on the point in time when the tick interrupts the code execution, the delay time can vary.
When the interrupt occurs before line 2 was executed, a task switch is performed and the time stop in line 3 is executed.
When the code continues to run, a yield is performed in line 5.
After the context switch, Task 1 continues in line 2 and another context switch is performed. 
So actually two context switches are measured. 
Another scenario which is theoretically possible is that the tick interrupts the test program right between line 3 and 4. 
In that case, the tick would not be visible in the graph.
Moreover, the delay can be caused by loading data needed by the \ac{ISR} to the cache.
\par
Further, small spike clusters which cause a jitter of about 35 ns are visible during the complete test.
Those cannot be explained by the test setup of the software, so it seems likely that they are caused by hardware, e.g. the cache mechanism.  
The delays observed in this experiments are also valid for the following experiments.
 
\subsection{Task Switching Results LinuxRT} 
The mean value for task switching in LinuxRT is 1.46 us and the standard deviation is 131 ns (9.0 \%).
The higher standard deviation is shown nicely in the upper graph of figure \ref{fig_tast_switching_result_linux}. 
Moreover, the timer tick is clearly recognizable every 10 ms.
Its maximum value is 18.945 us whereas the lower values are pending around 10 us.
Therefore, the maximum tick overhead is 17.485 us.
The reasons for the different tick timer delays can not be clearly determined because no appropriate tracing tools were available.
Besides the different timer tick entry points and the cache mechanism, the reasons for different latencies will be discussed later (refer to section \ref{ss_jitter_in_linux}). 
As in FreeRTOS, small spike clusters are visible during the complete test.
Because of the higher noise, they do not become as apparent as in the FreeRTOS measurement.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/task_switching_results_measurements_cfg6_int_saves}
	\end{center}
	\caption{Task switching result LinuxRT} \label{fig_tast_switching_result_linux}
\end{figure}

The mean task switching latency in LinuxRT is 6.1 times higher than in FreeRTOS.
This is mainly due to the implementation of the yield function in both systems.
Whereas the yield function in Linux causes a hierarchy of function calls, the yield in FreeRTOS is implemented as macro.   

\section{Semaphore Results}
This sections summarizes the results for the single mutex latency, the semaphore shuffling time and the event signaling. 
As the plots look very similar to section \ref{s_task_switching_results}, only the resulting values will be presented.
The single semaphore experiment shows the overhead arising when no other task interrupts an execution code secured by semaphores. 
The semaphore shuffling experiment defined by Kar quantifies the semaphore overhead when a second task tries to access data which is locked by a semaphore. 
The last experiment measures the latency which is caused by event synchronization. 
Concretely, this means waking up a task by signaling a binary semaphore in FreeRTOS or a condition variable in Linux.
\par
In time critical applications, the choice of the synchronization method can be crucial.
For example, sometimes the overhead of priority inheritance can be avoided by specific application design.

\subsection{Semaphore Results FreeRTOS}
All semaphore functions in FreeRTOS are based on message queues.
Mutexes support priority inheritance compared to the other kind of semaphores provided in FreeRTOS.
Therefore, the overhead when working with mutexes is slightly higher. 
The results of this measurement are presented in table \ref{tab_results_semaphores_freertos}. 

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l||l|l|l|}
			\hline
				Test 											& Single 		& Shuffle 	& Binary 	  \\
				\hline 
				Mean  										& 257 ns		& 1.912 us	& 846 ns	  \\
			  \hline
			  Std. deviation (total)	  & 15.2 ns		& 31 ns			&	4 ns		  \\
			  \hline
			  Std. deviation (\%)  			& 5.9 \%		& 1.64 \%		&	0.47 \%   \\ 
			  \hline
			  Max. tick delay	(total)		& 4.569 us	& 5.166 us	&	1.3 us    \\
				\hline
				Max. tick delay - mean		&	4.312	us	&	3.254	us	&	0.454 us  \\
			\hline
		\end{tabular}
	\caption{Results of the semaphore benchmarking in FreeRTOS}
	\label{tab_results_semaphores_freertos}
\end{table}

As already mentioned, the causes of delays are summarized in section \ref{ss_task_switching_results_freertos}.

\subsection{Semaphore Results LinuxRT}
The Linux implementation of the tests uses the POSIX \ac{API}.
It should be mentioned that the semaphores in this test are not configured to use priority inheritence. 
Comparing the mean values (see table \ref{tab_results_semaphores_linux}), the single semaphore test scores as well as the one in FreeRTOS.
Still, the standard deviation is much higher (46 \% compared to 5.9 \%).
The semaphore shuffling time in LinuxRT is 3.3 times higher than in FreeRTOS.
This is mainly caused by two context switches which are part of this experiment (see table \ref{tab_sem_kar}). 
They are caused by the blocking at the mutex of task 2 in line 4 and by the call yield function of task 1 in line 6.
The switch takes about 3 us and therefore almost half of the time. 
Even when considering the task switch time in the experiment, the latency in LinuxRT is still higher than in FreeRTOS.
The same problem arises for the event signaling experiment which contains one context switch.
Here, FreeRTOS is 3.01 times faster than Linux. 

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l||l|l|l|}
			\hline
				Test 											& Single 		& Shuffle 	& Binary 	  \\
				\hline 
				Mean  										& 252 ns		& 6.342 us	& 2.554 us  \\
			  \hline
			  Std. deviation (total)	  & 116 ns		& 286 ns		&	181 ns	  \\
			  \hline 
			  Std. deviation (\%)  			& 46 \%			& 4.51 \%		&	7,09 \%   \\ 
			  \hline
			  Max. tick delay	(total)		& 29.144 us	& 27.084 us	&	13.629 us \\
				\hline
				Max. tick delay - mean		&	28.892 us	&	20.742 us	&	11.075 us	\\
			\hline
		\end{tabular}
	\caption{Results semaphore tests in LinuxRT}
	\label{tab_results_semaphores_linux}
\end{table}

\section{Message Passing Time}
Message passing is another mechanism of task synchronization.
The algorithm (see table \ref{tab_message_passing}) also provides some points where a task switch can cause faulty results.
One possible error is that the execution is interrupt between line 2 and 3.
This causes one additional task switching delay because task 2 is not schedulable, so task 1 will be rescheduled.
Another error can happen when task 2 is interrupted before blocking on the queue. 
This causes the message to be already available in the queue after task 1 yields the processor. 
Therefore, task 2 does not block but retrieves the message immediately what takes less time than the original procedure.

\subsection{Message Passing in FreeRTOS}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/message_passing_latency_FreeRTOS_start_end}
	\end{center}
	\caption{Binary semaphore result FreeRTOS} \label{fig_message_passing_free}
\end{figure}
The mean time for passing one message with the content of one byte is 901 ns. 
The standard deviation is 24.9 ns (2.76 \%) and the maximum peak occuring while the timer interrupt is active is 2.531 us.
This makes a total overhead of 1,63 us. 
The lower graph (figure \ref{fig_message_passing_free}) clearly shows the error cases discussed above.  

\subsection{Message Passing in LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/message_passing_latency_measurements_cfg6_int_saves}
	\end{center}
	\caption{Condition variable result LinuxRT} \label{fig_message_passing_linux}
\end{figure}

As already mentioned, the SysV \ac{API} was used for this experiment, not the POSIX \ac{API} as it is not supported by the platform.
The mean time for passing one message in LinuxRT with the content of one byte is 5.135 us (see figure \ref{fig_message_passing_linux}). 
The standard deviation is 268 ns (5.22 \%) and the maximum peak occuring while the timer interrupt is active is 18.657 us.
This makes a total overhead of 13.522 us. 
The mean message passing latency in FreeRTOS is 5.7 times faster than in LinuxRT.

\section{Deadlock Breaking Time}
The detailed algorithm to obtain the deadlock breaking time is described in table \ref{tab_deadlock}.
This time gives an orientation on the time the operation system needs to resolve a priority inversion by the implemented priority inheritance algorithm.
As usually only one active task is running in this experiment, the timer tick causes an unnecessary context switch which results in the same task being continued.
This experiment has similarities to the semaphore shuffling but also includes the priority inheritance component.

\subsection{Deadlock Breaking Time in FreeRTOS}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/deadlock_results_FreeRTOS_start_end}
	\end{center}
	\caption{Deadlock breaking time FreeRTOS} \label{fig_deadlock_result_free}
\end{figure}
The mean time for the deadlock breaking time in FreeRTOS is 2.113 us. 
The standard deviation is 34.5 ns (1.63 \%) and the maximum peak occuring while the timer interrupt is active is 3.854 us.
This makes a total overhead of 1,74 us. 

\subsection{Deadlock Breaking Time in LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/deadlock_results_measurements_cfg6_int_saves}
	\end{center}
	\caption{Deadlock breaking time LinuxRT} \label{fig_deadlock_result_linux}
\end{figure}
The mean time for the deadlock breaking time in LinuxRT is 20.68 us. 
The standard deviation is 643 ns (3.11 \%) and the maximum peak occuring while the timer interrupt is active is 37.146 us.
This makes a total overhead of 16.466 us.
The LinuxRT implementation is 9.79 times slower than the same implementation in FreeRTOS.
[results from give].
 
\section{Interrupt Latency}\label{s_interrup_latency} 
The interrupt latency depends not only on the underlying hardware but also on the handling by the operation system.
While the only interrupt which is being handled directly by FreeRTOS is the timer tick, Linux supports more advanced handling of interrupts. 
Therefore, the analysis of the interrupt latency in Linux is more complex (see section \ref{ss_interrupt_latency_results_in_linux}).

\subsection{Interrupt Latency Results in FreeRTOS}
As there is no \ac{OS} overhead in the interrupt handling in FreeRTOS, it is very fast and predictive.
The time to handle the interrupt internally and turn on an \ac{LED} from the \ac{ISR} takes no more than 750 ns (see figure \ref{fig_interrupt_latency_freertos}). 
The mean value is 727.71 ns and the standard deviation 4.775 ns (0.66 \%). 
When the \ac{LED} is turned on from a user task which is unblocked by the \ac{ISR}, the mean time increases to 933.66 ns.
The standard deviation for this experiment is 41.01 ns (4.39 \%) and the maximum measured value is 1.163 us.
  
\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/isr_free}
	\end{center}
	\caption[Interrupt latency FreeRTOS]{Interrupt latency FreeRTOS, resolution: 1 us} \label{fig_interrupt_latency_freertos}
\end{figure}

\subsection{Interrupt Latency Results in Linux}\label{ss_interrupt_latency_results_in_linux}
The interrupt handling in Linux is quiet complex and many different cases have to be considered. 
As already mentioned, the \acp{ISR} in LinuxRT are threaded to reduce interrupt delays for critical tasks. 
Obviously, this increases the total interrupt latency.
To investigate how much the threading influences the interrupt latency, the tests were also run on a standard Linux platform as well.
Further, the tests were also performed under load condition as it is desirable to run standard [Wort?] applications on LinuxRT, not only real-time applications. 
This experiment shows very well the influence of \ac{I/O} bound load on the Linux system.

\subsubsection{The fastest Interrupt Handling in Linux}
The first test which was performed is very similar to the FreeRTOS test. 
It tests the standard Linux system's interrupt latency.
This means that the interrupts are handled non-threaded and the \ac{LED} is lightened in the \ac{ISR} directly.
The setup was tested on a standard Linux system.
The mean value for 50000 measurements is 1.812 us with a standard deviation of 433.06 ns (22.90 \%). 
This is only 2.5 times higher than in FreeRTOS. 
The maximum measured value is 3.51 us.
When put under stress by a constant SCP transfer, the value increases to a mean value of 2.679 us and a standard deviation of 1.30 us (48.53 \%).
The maximum value rises to 29.5 us.
\par
When the same experiment was repeated for LinuxRT, logically, the resulting values are much higher.
The mean value is 10.027 us with a standard deviation of 987.36 ns. 
The maximum measured value is 17.05 us.
After stressing the system, the mean value stays almost constant at 10.486 us, but the standard deviation rises to 4.496 us (42.88 \%) and the maximum recorded value to 90 us.

\subsubsection{Interrupt Handling and User Processes in LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5]{inputs/pictures_ch3/rt_isr_user}
	\end{center}
	\caption[Interrupt latency LinuxRT]{Interrupt latency LinuxRT, resolution 5 us} 	\label{fig_interrupt_latency_linuxrt}
\end{figure}
Usually, programmer prefer to write programs in user space and not kernel modules. 
Therefore, in this test a typical user thread scenario is simulated.
A thread is waiting for an interrupt to occur and then issues a write command to turn off an \ac{LED}. 
Both experiments were repeated for mainline Linux and LinuxRT. 
The results were recorded by an oscilloscope.
In the figures (\ref{fig_interrupt_latency_linuxrt} - \ref{fig_interrupt_latency_linux_load}), the upper channel shows the pulse generator which triggers the interrupt.
The lower channel shows the reaction of the \ac{OS}.  

The mean value measured for the user task when using LinuxRT is 23.527 us. 
The standard deviation is 1.629 us (6.92 \%). 
In figure \ref{fig_interrupt_latency_linuxrt}, the latencies for the experiment are shown.
Evidently, there is a high jitter although no load was put on the system. 
The maximum latency is 40.03 us. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/rt_isr_user_load}
	\end{center}
	\caption[Interrupt latency LinuxRT with load]{Interrupt latency LinuxRT with load, resolution 15 us} \label{fig_interrupt_latency_linuxrt_load}
\end{figure}

Figure \ref{fig_interrupt_latency_linuxrt_load} shows the same experiment when the system was stressed with network load. 
The mean value for this experiment is 27.375 and the standard deviation is 6.676 us (24.39 \%).
The maximum value here is 53.740 us, it is 34 \% higher than without load.

\subsubsection{Interrupt Handling and User Processes in Mainline Linux}
To see how significant the differences between LinuxRT and the mainline kernel are, the same experiment is also repeated using the mainline kernel. 
\par
The mean value measured for the user task is 12.350 us. 
The standard deviation is 1.250 us (10.12 \%). 
In figure \ref{fig_interrupt_latency_linux}, the latencies for the experiment are shown.
The maximum value is at 26.47 us. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/linux_isr_user}
	\end{center}
	\caption[Interrupt latency standard Linux]{Interrupt latency standard Linux, resolution 5 us} \label{fig_interrupt_latency_linux}
\end{figure}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/linux_isr_user_load}
	\end{center}
	\caption[Interrupt latency standard Linux with load]{Interrupt latency standard Linux with load, resolution 5 us} \label{fig_interrupt_latency_linux_load}
\end{figure}
Figure \ref{fig_interrupt_latency_linux_load} shows the same experiment stressed with \ac{I/O} load.  
As in LinuxRT, the maximum registered value is unacceptably large: 90.92 us and more than three times higher than without load.
Moreover, an even higher value could be observed in other experiments.
The mean value is at 15.853 us and the standard deviation at 6.01 us (37.91 \%).
From this experiment, the strength of LinuxRT becomes visible as the high network load can be preempted by real-time tasks with higher priority.

\section{Preemption Time}
The situation captured in this benchmark is often occurring in real-time applications.  
The preemption time is related to the interrupt latency, but measures the time that it takes to interrupt a low priority task and then continue a high priority task.
Another difference is that the test program does not record the time it takes to communicate with the peripheral devices (trigger interrupt, turning on \ac{LED}).
The \ac{GPIO} interrupt is triggered by the interrupt generator every 5 ms.

\subsection{Preemption Time in FreeRTOS}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/preemption_time_FreeRTOS_start_end}
	\end{center}
	\caption{Preemption result FreeRTOS} \label{fig_preemption_result_free}
\end{figure}

The mean value measured for the preemption time is 2.111 us. 
The standard deviation is 75 us (3.55 \%). 
In figure \ref{fig_preemption_result_free}, the results of the experiment are shown.
The maximum value is 2.626 us. 
Because the results were recorded only every 5 ms, it is likely that one peak in the in the upper plot results from the test being interrupted by the timer tick. 
The other peak results from the uninterrupted measurement. 

\subsection{Preemption Time in LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/preemption_time_measurements_cfg6_int_saves}
	\end{center}
	\caption{Preemption result LinuxRT} \label{fig_preemption_result_linux}
\end{figure}
The mean value measured for the preemption time in LinuxRT is 15.370 us and the standard deviation is 9.170 us (59.66 \%). 
In figure \ref{fig_preemption_result_linux}, the results of the experiment are shown.
The maximum value is 109.968 us.
This results in a total maximum overhead of 94.598 us which is one of the highest measured in all tests. 
Obviously, the overhead results from using the interrupts.
Its source could not be further localized as the tracing possibilities were limited and the installation of proper tracing tools would extend the limits of this work. 

It is also not obvious why there are three peaks and not only two line in the FreeRTOS measurement.
The mean time is 7.28 times higher than in FreeRTOS.
 
\section{Jitter}
As discussed in section \ref{ss_jitter2}, jitter can have a crucial influence on the execution time of an application.
Formula \ref{eq_indet_app} describes the worst case scenario:\\
$t_{jitter} = t_{tick} + t_{int} + t_{cache} + t_{swap} + t_{daem}$ \\
Based on the results, the single parameters will be discussed for the operation systems under test. 
As already mentioned, the cache time will not be handled because cache performance analysis is such a complex topic that it would exceed the limitations of this work.
The results presented are only valid for the current system setup and can be different for setups with e.g. multi-core support.
As already mentioned, the jitter in FreeRTOS is very predictive and can be reduced to the timer tick only.
For Linux, the overall system performance is stable when the application is designed correctly and no \ac{I/O} load is applied to the system.

\subsection{Jitter in FreeRTOS}
As expected from a light-weight \ac{RTOS}, the jitter in FreeRTOS is small and predictive.
$ t_{int}$,  $t_{swap} $ and $t_{daem}$ are not available and therefore set to zero. 
$ t_{tick} $ was measured throughout the experiments. 
The largest value recorded is 4.312 us in the single semaphore experiment.  
Therefore, $ t_{tick} $ will be set to 4.312 us.
\par
The final $t_{jitter}$ results to $ t_{tick} $ and 4.312 us for FreeRTOS.
The equation shows impressively how low \ac{OS} jitter is and consequently, how deterministic the system behaves.  
 
\subsection{Jitter in Linux}\label{ss_jitter_in_linux}
The jitter sources in Linux have to be investigated more closely than the ones in FreeRTOS.
As described in section \ref{ss_linuxRT_application_design}, some significant jitter sources can simply be eliminated by careful system and application design.

\subsubsection{Page Swapping} 
 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.6] 			{inputs/pictures_ch3/mutex_shuffle_measurements_cfg2_linux_printf_release}
	\end{center}
	\caption{Mutex shuffling results without applying RAM lock} \label{fig_no_mlock_mutex_shuffling}
\end{figure}
\begin{figure}[htb]

	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.6] 			{inputs/pictures_ch3/deadlock_results_measurements_cfg2_linux_printf_release}
	\end{center}
	\caption{Deadlock breaking results without applying RAM lock} \label{fig_no_mlock_deadlock}
\end{figure}

As already mentioned, page swapping can cause delays and should be turned off for time critical applications. 
The benchmark set has been performed without \ac{RAM} locking as well to investigate the result of the call to mlockall() function.
The effects of page swapping are visualized for the mutex shuffling (figure \ref{fig_no_mlock_mutex_shuffling}) and the deadlock breaking benchmark (figure \ref{fig_no_mlock_deadlock}).
The figures clearly show, that page swapping has a negative effect on the application performance.
Table \ref{tab_results_ram_lock} shows a comparison for both benchmarks with and without the \ac{RAM} lock.

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l||l||l||l|l|}
			\hline
				Test 											& Shuffle lock & Shuffle no lock & Deadlock lock & Deadlock no lock \\
				\hline 
				Mean  										& 6.342 us		 & 6.447 us 			 & 20.68 us			 & 20.73 us					\\
			  \hline
			  Std. deviation (total)	  & 286 ns			 & 527 ns 				 & 643 ns				 & 913 ns						\\
			  \hline 
			  Std. deviation (\%)  			& 4.51 \%			 & 8.17 \% 				 & 3.11 \% 			 & 4.40 \%					\\ 
			  \hline
			  Max. tick delay	(total)		& 27.084 us	 	 & 30.542 us 			 & 37.146 us		 & 60.473 				\\
			\hline
		\end{tabular}
	\caption{RAM lock effect on Deadlock breaking time and semaphore shuffling time}
	\label{tab_results_ram_lock}
\end{table}

The test shows that the \ac{RAM} lock has only a slight effect on the mean result, but the standard deviation increases especially for applications with a small mean time. 
Further, the maximum tick delay highly increases for this configuration.

\subsubsection{Daemons}
The number of daemons in Linux can be controlled actively, especially in embedded distributions, the number of daemons is limited by default.
The daemons available in the custom system are an \ac{SSH} and an \ac{FTP} daemon.
Both require an Ethernet connection and as shown in section \ref{s_interrup_latency}, this is not acceptable for a real-time application.
Consequently, both daemons are not allowed to actively communicate via network while a critical application is running.

\subsubsection{CPU Bound Load}
\ac{CPU} bound load means that a process is performing a task where no \ac{I/O} communication is necessary.
These can be for example processes which are collecting system data for statistical purposes. 
Further, bottom halves of \acp{ISR} can also perform \ac{CPU} bound actions based on data which was previously collected during the top half of the interrupt.
As this kind of load always depends on correct priority settings, real-time tasks should not be affected by \ac{CPU} load. 
An experiment where \ac{CPU} load was simulated by the Stress program shows no difference in behavior between the stressed and the non-stressed system.

\subsubsection{Timer Tick and Interrupt Delays}
The highest captured value for the timer tick which can for sure be defined as $t_{tick}$ is 28.892 us in the single semaphore benchmark.
The tick value is significantly larger than in FreeRTOS because Linux is performing more operations than rescheduling and updating of the internal timers. 
For example, it checks and invokes \ac{RCU} callbacks.
Moreover, the hierarchy of nested function calls has a high impact on the over-all performance of the system. 
The interrupt latency benchmarks proves that LinuxRT has a lower throughput but behaves more deterministic under heavy \ac{I/O} load. 
The maximum jitter value is almost twice as high for mainline Linux than for LinuxRT.
Still, the preemption benchmark shows infrequently occurring peaks of up to 110 us. 
%As clearly shown in the interrupt latency and preemption benchmarks, Linux does not behave deterministically when put under high \ac{I/O} load.
Before using LinuxRT in real-time applications in the current configuration, the cause of this behavior should be determined.
%The consequence is that LinuxRT should not be used for real-time applications in the current configuration.
Yet, this does not mean that LinuxRT cannot be used for real-time applications at all.
It is possible to use a multi-core configuration and process interrupts with only a number of dedicated \acp{CPU}.
This would be even possible for the current hardware platform as the board inherits a dual core processor.
Further, there are a few mechanisms which propose ways to work around the timer tick.
Lee and Park introduce a technique to improve real-time performance by predicting the timer interrupt \cite{lee:dltfirtpoelbpoti}.

\section{Summary}
This section briefly summarizes the results which were presented throughout this chapter and gives a final comparison between the two operation systems under test.
\par
Table \ref{tab_summary} gives an overview over the collected results.
In the final right column, the factor between the mean FreeRTOS and mean LinuxRT value is calculated. 

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l||l|l||l|l||l|}
			\hline
				Test & \multicolumn{2}{|c||}{FreeRTOS} & \multicolumn{2}{|c||}{Linux}	& Factor				 	 \\
				\hline 
																						& mean			 & std. dev. 		 & mean			  & std. dev. 	&			 \\
				\hline 
				Task Switching											& 238 ns	 	 & 1.6 \%		 		 & 1.46 us	  & 9.0 \%			&	6.1	 \\
			  \hline
			  Semaphore single									  & 257 ns		 & 5.9 \%  			 & 252 ns		  &	46 \% 			&	0.98 \\
			  \hline 
			  Semaphore Shuffling 								& 1.912 us 	 & 1.64 \% 			 & 6.342 us  	&	4.51 \%			&	3.32 \\ 
			  \hline
			  Event Signaling											& 846 ns		 & 0.47 \% 			 & 2.544 us		& 7.09 \%			&	3.01 \\
			  \hline
			  Message Passing											& 901 ns		 & 2.76 \% 			 & 5.135 us		& 5.22 \%			&	5.7  \\
			  \hline
			  Deadlock Breaking										& 2.113 us	 & 1.63 \% 			 & 20.68 us		& 3.11 \%			&	9.79 \\
			  \hline
			  Interrupt Latency (fast)						& 727.71 ns	 & 0.66 \% 			 & 1.812 us		& 22.90 \%		&	2.49 \\
			  \hline
			  Interrupt Latency (user), RT				& 933.66 ns	 & 4.39 \% 			 & 23.527 us  & 6.92 \%			&	25.19 \\
			  %\hline
			  Linux mainline kernel								& 					 &  			 			 & 12.350 us	& 10.12 \%		&	13.23 \\
			  \hline
			  Preemption													& 2.111 us	 & 3.55 \% 			 & 15.370 us	& 59.66 \%		&	7.28 \\
			\hline
		\end{tabular}
	\caption{Result summary for the benchmarks}
	\label{tab_summary}
\end{table}

The results point out the strengths and weaknesses of LinuxRT compared to FreeRTOS.
Therefore, it becomes clear which kind of applications can be served by a LinuxRT system and where it gets to its limits.
Linux suffers especially from long interrupt latency times due to its complex interrupt handling.
Still, when using the non-threaded interrupt handling for LinuxRT, it is possible to handle \ac{I/O} inputs from user space at a mean time of 12.350 us. 
When considering the worst case time for interrupt handling, an interrupt could be processed within 30 us.
The worst-case processing time of a threaded interrupt with load is at 54 us.
This means that the jitter arising from putting the system under network load is not even 25 us.
Practically, user space programs should be written when the processing of sensor data needs complex calculations which for example include usage of an \ac{FPU}.
Simple sensor-actor applications can be processed directly in the \ac{ISR}.
The worst-case time of 4 us without additional interrupt load is very fast and barely slower than in FreeRTOS. 
Obviously, the maximum latency of the application $t_{app}$ has to be considered as well.
\par
Besides the interrupt handling, the other Rhealstone benchmarks give an idea about the overall performance of LinuxRT compared to FreeRTOS and correspond to $t_{os}$. 
Especially task switching, message passing and deadlock breaking causes large delays in LinuxRT.
Applications with a high number of tasks and many task switches suffer from slow context switching in Linux. 
This is a point where optimization could increase performance as Linux and FreeRTOS are using the same scheduling algorithm for real-time tasks.  
The other aspect in which LinuxRT performs really bad is resolving deadlock situations by using priority inheritance.
The complexity of the POSIX compliant \ac{API} is particularly recognizable at this point.
One solution to this problem is an application design which does not allow such situations. 
Yet, it is sometimes necessary to create applications where priority inversion can occur because another solution would be too complex.
Depending on the real-time requirements, LinuxRT can fail because it needs about 20 us longer to resolve a deadlock. 
A better solution to this problem would be the design of a semaphore lightweight \ac{API} for real-time applications.
As semaphores and mutexes are a very common memory access synchronization technique, this would cause some significant improvement in performance.
The downside of this approach is a higher memory footprint of the operation system.
Further, it was shown that event signaling is twice as fast than message passing in LinuxRT for the used \acp{API}.
Because both methods implement similar functions, it is faster to use event signaling and shared memory between two threads than passing messages.
In FreeRTOS, the related \acp{API} calls perform almost the same.
This shows the correctness of the results as the semaphore implementation in FreeRTOS is completely based on message queues.
\par
Besides the application design, the system has to be set up carefully to minimize $t_{jitter}$. 
It is naive to assume, a complex \ac{OS} like LinuxRT would work for real-time applications without being tuned or without restrictions although this would the most desirable situation.
Consequently, the jitter has to be either considered during application design or removed before running the real-time application.
The possibilities strongly depend on the underlying hardware.
The current single core configuration does not allow the unlimited usage of \ac{I/O} devices because this has a strong impact on the real-time performance. 
Still, it has to be pointed out that the worst case value of $t_{int}$ is not even 25 us larger than in the idle case when putting the LinuxRT system under network load 
This time is acceptable for a large group of real-time applications.
It is a positive result after testing the RT patch, but the programmer still needs to be aware that this worst-case value is only valid for the current system configuration. 
The configuration does include the hardware, the \ac{OS} setup, the interrupts generated by the pulse generator and the network load as stress factor.
Other jitter factors like $t_{swap}$ and unnecessary daemons were eliminated if possible.
Another hardware setup which exploits the hardware provided by the evaluation platform is a dual core system. 
Linux has the advantage of assigning an \ac{CPU} affinity to special \acp{IRQ}.
This means that $t_{tick}$ and $t_{int}$ can be completely moved to another \ac{CPU}, so that time-critical tasks can run on one specific \ac{CPU} without being disturbed.
By doing this, $t_{jitter}$ is nearly eliminated so that mainly $t_{det}$ remains as execution time.
The downside of this method is that one \ac{CPU} is heavily loaded by communicating with \ac{I/O} devices while the other one can only run preemptable \ac{CPU} bound processes.
Although $t_{jitter}$ can be minimized in the introduced method, LinuxRT is barely used for real-time applications in industry.
This is mainly due to the complexity of the system which makes it hard to meet necessary verification demands.

%\subsection{Single Mutex Results}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_single_FreeRTOS_start_end}
%	\end{center}
%	\caption{Mutex in single task result FreeRTOS} \label{fig_mutex_single_result_free}
%\end{figure}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_single_measurements_cfg6_int_saves}
%	\end{center}
%	\caption{Mutex in single task result LinuxRT} \label{fig_mutex_single_result_linux}
%\end{figure}

%\subsection{Semaphore Shuffling}
%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_shuffle_FreeRTOS_start_end}
%	\end{center}
%	\caption{Mutex shuffling result FreeRTOS} \label{fig_mutex_shuffle_result_free}
%\end{figure}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_shuffle_measurements_cfg6_int_saves}
%	\end{center}
%	\caption{Mutex shuffling result LinuxRT} \label{fig_mutex_shuffle_result_linux}
%\end{figure}

%\subsection{Event Signaling}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/bin_semaphore_FreeRTOS_start_end}
%	\end{center}
%	\caption{Binary semaphore result FreeRTOS} \label{fig_bin_sem_result_free}
%\end{figure}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/cond_var_results_measurements_cfg6_int_saves}
%	\end{center}
%	\caption{Condition variable result LinuxRT} \label{fig_cond_var_result_linux}
%\end{figure}