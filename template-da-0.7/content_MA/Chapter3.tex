\chapter{Results}\label{ch_results}
In this chapter, the results from the previously described experiments are presented.
Moreover, the indeterministic aspects of the are further investigated for both operation systems. 
The main observation is - as expected - that LinuxRT is usually slower than FreeRTOS and has a higher jitter. 
This is especially recognizable when the benchmarks become more complex, for example for the deadlock breaking time or for the preemption time.
To create the same initial situation for every benchmark, the Linux operation system was rebooted every time before performing a test.

\section{Boot Time}
This section mainly shows the possible decrease of the boot time in Linux as there was no need to further improve the start up time of FreeRTOS.

\subsection{Boot Time of FreeRTOS}
The boot time of FreeRTOS has been quantified by formula \ref{eq_t_boot_free}.
As mentioned before, there is no need to optimize the FreeRTOS boot time, so the final $ t_{boot}^{free} $ is 1,1 s.

\subsection{Boot Time of Linux}
The boot time in Linux $ t_{boot}^{linux} $ was initially 20 seconds when using the boot configuration with a custom kernel from Xilinx (see \ref{ss_booting_in_linux}). 
With the new boot configuration described in section \ref{ss_booting_in_linux} the boot time could be reduced to 7.5 seconds. 
One main factor is the \ac{FPGA} configuration of the \ac{FSBL} which takes 11 seconds in the original configuration.
The change of the file system has an impact of 2 seconds as well but this is a minor factor as the \ac{UBI} gets slower with increasing memory. 
Extensive use of the system caused for example a command history and further cache data to be stored and checked on start-up.
This leads to an increase of the boot time to 9 seconds when the series of benchmarks was finished.
Yet, it is still possible to load the file system in read-only mode which decreases $ t_{boot}^{linux} $ to 5.7 s.
This can be for example used in a running systems where no writing of data to the file system is necessary.
In configuration mode, the system can be easily booted in read-write mode so that existing applications or configuration files can be updated. 
The result of this experiment is a boot time which was decreased by 55 \% in read-write mode and by 71.5 \% in read-only mode.

\section{Task Switching Results}\label{s_task_switching_results}
The measurements are presented in two different graphs. 
The upper one always shows the number of the occurrence of one latency over time. 
This curve gives a good overview over the statistic and deviation of the measurement.
The lower picture shows the latencies plotted over time. 
This curve shows time dependent events like the timer tick.  
%Further, this presentation of the results is not only valid for the task switching time but for all results presented in the following.

\subsection{Task Switching Results FreeRTOS} \label{ss_task_switching_results_freertos}
\begin{figure}[hbt]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/task_switching_debug_FreeRTOS_start_end}
	\end{center}
	\caption{Task switching result FreeRTOS} \label{fig_tast_switching_result_free}
\end{figure}
As the red color in figure \ref{fig_tast_switching_result_free} shows, the mean value for FreeRTOS task switching is 238 ns. 
The standard deviation for the test set is 3.8 ns (1.6 \%).
The peaks at the time of 0.1 and 0.2 us are showing the active timer tick. 
The probability to capture the timer tick in this experiment is very high because a large density of the test program code is included into the time measurement.
The maximum value measured for the tick is 1.059 us.
When the task switching time is subtracted from this value, the resulting tick overhead is 821 ns.
The reason for the different tick times (1.059 us and 0.752 us) cannot be clearly verified, as no proper tracing tools were available. 
Yet, some assumptions can be made on the behavior of the system. 
In table \ref{tab_task_switching}, the test algorithm is described. 
Depending on the point in time when the tick interrupts the code execution, the delay time can vary.
When the interrupt occurs before line 2 was executed, a task switch is performed and the time stop in line 3 is executed.
When the code continues to run, a yield is performed in line 5.
After the context switch, Task 1 continues in line 2 and another context switch is performed. 
So actually two context switches are measured. 
Another scenario which is theoretically possible is that the tick interrupts the test program right between line 3 and 4. 
In that case, the tick would not be visible in the graph.
Moreover, the delay can be caused by loading data needed by the \ac{ISR} to the cache.
\par
Further, small spike clusters which cause a jitter of about 35 ns are visible during the complete test.
Those cannot be explained by the test setup of the software, so it seems likely that they are caused by hardware, e.g. the cache mechanism.  
The delays observed in this experiments are also valid for the following experiments.
 
\subsection{Task Switching Results LinuxRT} 
The mean value for task switching in LinuxRT is 1.46 us and the standard deviation is 131 ns (9.0 \%).
The higher standard deviation is shown nicely in the upper graph of figure \ref{fig_tast_switching_result_linux}. 
Moreover, the timer tick is clearly recognizable every 10 ms.
Its maximum value is 18.945 us whereas the lower values are pending around 10 us.
Therefore, the maximum tick overhead is 17.485 us.
The reasons for the different tick timer delays can not be clearly determined because no appropriate tracing tools were available.
Besides the different timer tick entry points and the cache mechanism, the reasons for different latencies will be discussed later (refer to section \ref{}). 
As in FreeRTOS, small spike clusters are visible during the complete test.
Because of the higher noise, they do not become as apparent as in the FreeRTOS measurement.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/task_switching_results_measurements_cfg6_int_saves}
	\end{center}
	\caption{Task switching result LinuxRT} \label{fig_tast_switching_result_linux}
\end{figure}

The mean task switching latency in LinuxRT is 6.1 times higher than in FreeRTOS.
This is mainly due to the implementation of the yield function in both systems.
Whereas the yield function in Linux causes a hierarchy of function calls, the yield in FreeRTOS is implemented as macro.   

\section{Semaphore Results}
This sections summarizes the results for the single mutex latency, the semaphore shuffling time and the event signaling. 
As the plots look very similar to section \ref{s_task_switching_results}, only the resulting values will be presented.
The single semaphore experiment shows the overhead arising when no other task interrupts an execution code secured by semaphores. 
The semaphore shuffling experiment defined by Kar quantifies the semaphore overhead when a second task tries to access data which is locked by a semaphore. 
The last experiment measures the latency which is caused by event synchronization. 
Concretely, this means waking up a task by signaling a binary semaphore in FreeRTOS or a condition variable in Linux.
\par
In time critical applications, the choice of the synchronization method can be crucial.
For example, sometimes the overhead of priority inheritance can be avoided by specific application design.

\subsection{Semaphore Results FreeRTOS}
All semaphore functions in FreeRTOS are based on message queues.
Mutexes support priority inheritance compared to the other kind of semaphores provided in FreeRTOS.
Therefore, the overhead when working with mutexes is slightly higher. 
The results of this measurement are presented in table \ref{tab_results_semaphores_freertos}. 

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l||l|l|l|}
			\hline
				Test 											& Single 		& Shuffle 	& Binary 	  \\
				\hline 
				Mean  										& 257 ns		& 1.912 us	& 846 ns	  \\
			  \hline
			  Std. deviation (total)	  & 15.2 ns		& 31 ns			&	4 ns		  \\
			  \hline
			  Std. deviation (\%)  			& 5.9 \%		& 1.64 \%		&	0.47 \%   \\ 
			  \hline
			  Max. tick delay	(total)		& 4.569 us	& 5.166 us	&	1.3 us    \\
				\hline
				Max. tick delay - mean		&	4.312	us	&	3.254	us	&	0.454 us  \\
			\hline
		\end{tabular}
	\caption{Results of the semaphore benchmarking in FreeRTOS}
	\label{tab_results_semaphores_freertos}
\end{table}

As already mentioned, the causes of delays are summarized in section \ref{ss_task_switching_results_freertos}.

\subsection{Semaphore Results LinuxRT}
The Linux implementation of the tests uses the POSIX \ac{API}.
It should be mentioned that the semaphores in this test are not configured to use priority inheritence. 
Comparing the mean values (see table \ref{tab_results_semaphores_linux}), the single semaphore test scores as well as the one in FreeRTOS.
Still, the standard deviation is much higher (46 \% compared to 5.9 \%).
The semaphore shuffling time in LinuxRT is 3.3 times higher than in FreeRTOS.
This is mainly caused by two context switches which are part of this experiment (see table \ref{tab_sem_kar}). 
They are caused by the blocking at the mutex of task 2 in line 4 and by the call yield function of task 1 in line 6.
The switch takes about 3 us and therefore almost half of the time. 
Even when considering the task switch time in the experiment, the latency in LinuxRT is still higher than in FreeRTOS.
The same problem arises for the event signaling experiment which contains one context switch.
Here, FreeRTOS is 3.01 times faster than Linux. 

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l||l|l|l|}
			\hline
				Test 											& Single 		& Shuffle 	& Binary 	  \\
				\hline 
				Mean  										& 252 ns		& 6.342 us	& 2.554 us  \\
			  \hline
			  Std. deviation (total)	  & 116 ns		& 286 ns		&	181 ns	  \\
			  \hline 
			  Std. deviation (\%)  			& 46 \%			& 4.51 \%		&	7,09 \%   \\ 
			  \hline
			  Max. tick delay	(total)		& 29.144 us	& 27.084 us	&	13.629 us \\
				\hline
				Max. tick delay - mean		&	28.892 us	&	20.742 us	&	11.075 us	\\
			\hline
		\end{tabular}
	\caption{Results semaphore tests in LinuxRT}
	\label{tab_results_semaphores_linux}
\end{table}

\section{Message Passing Time}
Message passing is another mechanism of task synchronization.
The algorithm (see table \ref{tab_message_passing}) also provides some points where a task switch can cause faulty results.
One possible error is that the execution is interrupt between line 2 and 3.
This causes one additional task switching delay because task 2 is not schedulable, so task 1 will be rescheduled.
Another error can happen when task 2 is interrupted before blocking on the queue. 
This causes the message to be already available in the queue after task 1 yields the processor. 
Therefore, task 2 does not block but retrieves the message immediately what takes less time than the original procedure.

\subsection{Message Passing in FreeRTOS}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/message_passing_latency_FreeRTOS_start_end}
	\end{center}
	\caption{Binary semaphore result FreeRTOS} \label{fig_message_passing_free}
\end{figure}
The mean time for passing one message with the content of one byte is 901 ns. 
The standard deviation is 24.9 ns (2.76 \%) and the maximum peak occuring while the timer interrupt is active is 2.531 us.
This makes a total overhead of 1,63 us. 
The lower graph (figure \ref{fig_message_passing_free}) clearly shows the error cases discussed above.  

\subsection{Message Passing in LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/message_passing_latency_measurements_cfg6_int_saves}
	\end{center}
	\caption{Condition variable result LinuxRT} \label{fig_message_passing_linux}
\end{figure}

As already mentioned, the SysV \ac{API} was used for this experiment, not the POSIX \ac{API} as it is not supported by the platform.
The mean time for passing one message in LinuxRT with the content of one byte is 5.135 us (see figure \ref{fig_message_passing_linux}). 
The standard deviation is 268 ns (5.22 \%) and the maximum peak occuring while the timer interrupt is active is 18.657 us.
This makes a total overhead of 13.522 us. 
The mean message passing latency in FreeRTOS is 5.7 times faster than in LinuxRT.

\section{Deadlock Breaking Time}
The detailed algorithm to obtain the deadlock breaking time is described in table \ref{tab_deadlock}.
This time gives an orientation on the time the operation system needs to resolve a priority inversion by the implemented priority inheritance algorithm.
As usually only one active task is running in this experiment, the timer tick causes an unnecessary context switch which results in the same task being continued.
This experiment has similarities to the semaphore shuffling but also includes the priority inheritance component.

\subsection{Deadlock Breaking Time in FreeRTOS}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/deadlock_results_FreeRTOS_start_end}
	\end{center}
	\caption{Deadlock breaking time FreeRTOS} \label{fig_deadlock_result_free}
\end{figure}
The mean time for the deadlock breaking time in FreeRTOS is 2.113 us. 
The standard deviation is 34.5 ns (1.63 \%) and the maximum peak occuring while the timer interrupt is active is 3.854 us.
This makes a total overhead of 1,74 us. 

\subsection{Deadlock Breaking Time in LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/deadlock_results_measurements_cfg6_int_saves}
	\end{center}
	\caption{Deadlock breaking time LinuxRT} \label{fig_deadlock_result_linux}
\end{figure}
The mean time for the deadlock breaking time in LinuxRT is 20.68 us. 
The standard deviation is 643 ns (3.11 \%) and the maximum peak occuring while the timer interrupt is active is 37.146 us.
This makes a total overhead of 16.466 us.
The LinuxRT implementation is 9.79 times slower than the same implementation in FreeRTOS.
[results from give].
 
\section{Interrupt Latency}\label{s_interrup_latency} 
The interrupt latency depends not only on the underlying hardware but also on the handling by the operation system.
While the only interrupt which is being handled directly by FreeRTOS is the timer tick, Linux supports more advanced handling of interrupts. 
Therefore, the analysis of the interrupt latency in Linux is more complex (see section \ref{ss_interrupt_latency_results_in_linux}).

\subsection{Interrupt Latency Results in FreeRTOS}
As there is no \ac{OS} overhead in the interrupt handling in FreeRTOS, it is very fast and predictive.
The time to handle the interrupt internally and turn on an \ac{LED} from the \ac{ISR} takes no more than 740 ns (see figure \ref{fig_interrupt_latency_freertos}). 
The mean value is 727.71 ns and the standard deviation 4.775 ns (0.66 \%). 
Even when the \ac{LED} is turned on from a task which is unblocked by the \ac{ISR}, the procedure is only 10 ns [?] slower.  
\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/isr_free}
	\end{center}
	\caption[Interrupt latency FreeRTOS]{Interrupt latency FreeRTOS, resolution: 1 us} \label{fig_interrupt_latency_freertos}
\end{figure}

\subsection{Interrupt Latency Results in Linux}\label{ss_interrupt_latency_results_in_linux}
The interrupt handling in Linux is quiet complex and many different cases have to be considered. 
As already mentioned, the \acp{ISR} in LinuxRT are threaded to reduce interrupt delays for critical tasks. 
Obviously, this increases the total interrupt latency.
To investigate how much the threading influences the interrupt latency, the tests were also run on a standard Linux platform as well.
Further, the tests were also performed under load condition as it is desirable to run standard [Wort?] applications on LinuxRT, not only real-time applications. 
This experiment shows very well the influence of \ac{I/O} bound load on the Linux system.

\subsection{The fastest Interrupt Handling in Linux}
The first test which was performed is very similar to the FreeRTOS test. 
It tests the standard Linux system's interrupt latency.
This means that the interrupts are handled non-threaded and the \ac{LED} is lightened in the \ac{ISR} directly.
The setup was tested on a standard Linux system.
The mean value for 50000 measurements is 1.812 us with a standard deviation of 433.06 ns (22.90 \%). 
This is only 2.5 times higher than in FreeRTOS. 
The maximum measured value is 3.51 us.
When put under stress by a constant SCP transfer, the value increases to a mean value of 2.679 us and a standard deviation of 1.30 us (48.53 \%).
The maximum value rises to 29.5 us.
\par
When the same experiment was repeated for LinuxRT, logically, the resulting values are much higher.
The mean value is 10.027 us with a standard deviation of 987.36 ns. 
The maximum measured value is 17.05 us.
After stressing the system, the mean value stays almost constant at 10.486 us, but the standard deviation rises to 4.496 us (42.88 \%) and the maximum recorded value to 90 us.

\subsection{Interrupt handling and User Processes}
Usually, programmer prefer to write programs in user space and not kernel modules. 
Therefore, in this test a typical user thread scenario is simulated.
A thread is waiting for an interrupt to occur and then issues a write command to turn off an \ac{LED}. 
Both experiments were repeated for mainline Linux and LinuxRT. 
The results were recorded by an oscilloscope.
In the figures (\ref{fig_interrupt_latency_linuxrt} - \ref{fig_interrupt_latency_linux_load}), the upper channel shows the pulse generator which triggers the interrupt.
The lower channel shows the reaction of the \ac{OS}.  

\subsubsection{LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/rt_isr_user}
	\end{center}
	\caption[Interrupt latency LinuxRT]{Interrupt latency LinuxRT, resolution 5 us} \label{fig_interrupt_latency_linuxrt}
\end{figure}
The mean value measured for the user task when using LinuxRT is 23.527 us. 
The standard deviation is 1.629 us (6.92 \%). 
In figure \ref{fig_interrupt_latency_linuxrt}, the latencies for the experiment are shown.
Evidently, there is a high jitter although no load was put on the system. 
The maximum latency is 40.03 us. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/rt_isr_user_load}
	\end{center}
	\caption[Interrupt latency LinuxRT with load]{Interrupt latency LinuxRT with load, resolution 15 us} \label{fig_interrupt_latency_linuxrt_load}
\end{figure}
Figure \ref{fig_interrupt_latency_linuxrt_load} shows the same experiment when the system was stressed with network load. 
The mean value for this experiment is 29.375 and the standard deviation is 6.202 us (21.11 \%).
The maximum value here is 126.836 us.

\subsubsection{Mainline Linux}
To see how significant the differences between LinuxRT and the mainline kernel are, the same experiment is also repeated using the mainline kernel. 
\par
The mean value measured for the user task is 12.350 us. 
The standard deviation is 1.250 us (10.12 \%). 
In figure \ref{fig_interrupt_latency_linux}, the latencies for the experiment are shown.
The maximum value is at 26.47 us. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/linux_isr_user}
	\end{center}
	\caption[Interrupt latency standard Linux]{Interrupt latency standard Linux, resolution 5 us} \label{fig_interrupt_latency_linux}
\end{figure}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.5] 
		{inputs/pictures_ch3/linux_isr_user_load}
	\end{center}
	\caption[Interrupt latency standard Linux with load]{Interrupt latency standard Linux with load, resolution 5 us} \label{fig_interrupt_latency_linux_load}
\end{figure}
Figure \ref{fig_interrupt_latency_linux_load} shows the same experiment stressed with \ac{I/O} load.  
As in LinuxRT, the maximum registered value is unacceptably large (90.92 us).
Moreover, an even higher value could be observed in other experiments.
The mean value is at 15.853 us and the standard deviation at 6.01 us (37.91 us \%).

\section{Preemption Time}
The situation captured in this benchmark is often occurring in real-time applications.  
The preemption time is related to the interrupt latency, but measures the time that it takes to interrupt a low priority task and then continue a high priority task.
Another difference is that the test program does not record the time it takes to communicate with the peripheral devices (trigger interrupt, turning on \ac{LED}).
The \ac{GPIO} interrupt is triggered by the interrupt generator every 5 ms.

\subsection{Preemption Time in FreeRTOS}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/preemption_time_FreeRTOS_start_end}
	\end{center}
	\caption{Preemption result FreeRTOS} \label{fig_preemption_result_free}
\end{figure}

The mean value measured for the preemption time is 2.111 us. 
The standard deviation is 75 us (3.55 \%). 
In figure \ref{fig_preemption_result_free}, the results of the experiment are shown.
The maximum value is 2.626 us. 
Because the results were recorded only every 5 ms, it is likely that one peak in the in the upper plot results from the test being interrupted by the timer tick. 
The other peak results from the uninterrupted measurement. 

\subsection{Preemption Time in LinuxRT}
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/preemption_time_measurements_cfg6_int_saves}
	\end{center}
	\caption{Preemption result LinuxRT} \label{fig_preemption_result_linux}
\end{figure}
The mean value measured for the preemption time in LinuxRT is 15.370 us and the standard deviation is 9.170 us (59.66 \%). 
In figure \ref{fig_preemption_result_linux}, the results of the experiment are shown.
The maximum value is 109.968 us.
This results in a total maximum overhead of 94.598 us which is one of the highest measured in all tests. 
Obviously, the overhead results from using the interrupts.
Its source could not be further localized as the tracing possibilities were limited and the installation of proper tracing tools would extend the limits of this work. 
It is also not obvious why there are three peaks and not only two line in the FreeRTOS measurement.
One possibility is that nested interrupt handling in Linux takes more time than in FreeRTOS. 
The mean time is 7.28 times higher than in FreeRTOS.
 
\section{Jitter}
As discussed in section \ref{ss_jitter2}, jitter can have a crucial influence on the execution time of an application.
Formula \ref{eq_indet_app} describes the worst case scenario:\\
$t_{jitter} = t_{tick} + t_{int} + t_{cache} + t_{swap} + t_{daem}$ \\
Based on the results, the single parameters will be discussed for the operation systems under test. 
As already mentioned, the cache time will not be handled because cache performance analysis is such a complex topic that it would exceed the limitations of this work.
The results presented are only valid for the current system setup and can be different for setups with e.g. multi-core support.
As already mentioned, the jitter in FreeRTOS is very predictive and can be reduced to the timer tick only.
For Linux, the overall system performance is stable when the application is designed correctly and no \ac{I/O} load is applied to the system.

\subsection{Jitter in FreeRTOS}
As expected from a light-weight \ac{RTOS}, the jitter in FreeRTOS is small and predictive.
$ t_{int}$,  $t_{swap} $ and $t_{daem}$ are not available and therefore set to zero. 
$ t_{tick} $ was measured throughout the experiments. 
The largest value recorded is 4.312 us in the single semaphore experiment.  
Therefore, $ t_{tick} $ will be set to 4.312 us.
\par
The final $t_{jitter}$ results to $ t_{tick} $ and 4.312 us for FreeRTOS.
The equation shows impressively how low \ac{OS} jitter is and consequently, how deterministic the system behaves.  
 
\subsection{Jitter in Linux}
The jitter sources in Linux have to be investigated more closely than the ones in FreeRTOS.
As described in section \ref{ss_linuxRT_application_design}, some significant jitter sources can simply be eliminated by careful system and application design.

\subsubsection{Page Swapping} 
 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.6] 			{inputs/pictures_ch3/mutex_shuffle_measurements_cfg2_linux_printf_release}
	\end{center}
	\caption{Mutex shuffling results without applying RAM lock} \label{fig_no_mlock_mutex_shuffling}
\end{figure}
\begin{figure}[htb]

	\begin{center}
		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.6] 			{inputs/pictures_ch3/deadlock_results_measurements_cfg2_linux_printf_release}
	\end{center}
	\caption{Deadlock breaking results without applying RAM lock} \label{fig_no_mlock_deadlock}
\end{figure}

As already mentioned, page swapping can cause delays and should be turned off for time critical applications. 
The benchmark set has been performed without \ac{RAM} locking as well to investigate the result of the call to mlockall() function.
The effects of page swapping are visualized for the mutex shuffling (figure \ref{fig_no_mlock_mutex_shuffling}) and the deadlock breaking benchmark (figure \ref{fig_no_mlock_deadlock}).
The figures clearly show, that page swapping has a negative effect on the application performance.
Table \ref{tab_results_ram_lock} shows a comparison for both benchmarks with and without the \ac{RAM} lock.

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l||l||l||l|l|}
			\hline
				Test 											& Shuffle lock & Shuffle no lock & Deadlock lock & Deadlock no lock \\
				\hline 
				Mean  										& 6.342 us		 & 6.447 us 			 & 20.68 us			 & 20.73 us					\\
			  \hline
			  Std. deviation (total)	  & 286 ns			 & 527 ns 				 & 643 ns				 & 913 ns						\\
			  \hline 
			  Std. deviation (\%)  			& 4.51 \%			 & 8.17 \% 				 & 3.11 \% 			 & 4.40 \%					\\ 
			  \hline
			  Max. tick delay	(total)		& 27.084 us	 	 & 30.542 us 			 & 37.146 us		 & 60.473 				\\
			\hline
		\end{tabular}
	\caption{RAM lock effect on Deadlock breaking time and semaphore shuffling time}
	\label{tab_results_ram_lock}
\end{table}

The test shows that the \ac{RAM} lock has only a slight effect on the mean result, but the standard deviation increases especially for applications with a small mean time. 
Further, the maximum tick delay highly increases for this configuration.

\subsubsection{Daemons}
The number of daemons in Linux can be controlled actively, especially in embedded distributions, the number of daemons is limited by default.
The daemons available in the custom system are an \ac{SSH} and an \ac{FTP} daemon.
Both require an Ethernet connection and as shown in section \ref{s_interrup_latency}, this is not acceptable for a real-time application.
Consequently, both daemons are not allowed to actively communicate via network while a critical application is running.

\subsubsection{CPU Bound Load}
\ac{CPU} bound load means that a process is performing a task where no \ac{I/O} communication is necessary.
These can be for example processes which are collecting system data for statistical purposes. 
Further, bottom halves of \acp{ISR} can also perform \ac{CPU} bound actions based on data which was previously collected during the top half of the interrupt.
As this kind of load always depends on correct priority settings, real-time tasks should not be affected by \ac{CPU} load. 
An experiment where \ac{CPU} load was simulated by the Stress program shows no difference in behavior between the stressed and the non-stressed system.

\subsubsection{Timer Tick and Interrupt Delays}
The highest captured value for the timer tick which can for sure be defined as $t_{tick}$ is 28.892 us in the single semaphore benchmark.
The tick value is significantly larger than in FreeRTOS because Linux is performing more operations than rescheduling and updating of the internal timers. 
For example, it checks and invokes \ac{RCU} callbacks.
Further, the hierarchy of nested function calls has a high impact on the over-all performance of the system. 
As clearly shown in the interrupt latency and preemption benchmarks, 

%\subsection{Single Mutex Results}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_single_FreeRTOS_start_end}
%	\end{center}
%	\caption{Mutex in single task result FreeRTOS} \label{fig_mutex_single_result_free}
%\end{figure}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_single_measurements_cfg6_int_saves}
%	\end{center}
%	\caption{Mutex in single task result LinuxRT} \label{fig_mutex_single_result_linux}
%\end{figure}

%\subsection{Semaphore Shuffling}
%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_shuffle_FreeRTOS_start_end}
%	\end{center}
%	\caption{Mutex shuffling result FreeRTOS} \label{fig_mutex_shuffle_result_free}
%\end{figure}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/mutex_shuffle_measurements_cfg6_int_saves}
%	\end{center}
%	\caption{Mutex shuffling result LinuxRT} \label{fig_mutex_shuffle_result_linux}
%\end{figure}

%\subsection{Event Signaling}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/bin_semaphore_FreeRTOS_start_end}
%	\end{center}
%	\caption{Binary semaphore result FreeRTOS} \label{fig_bin_sem_result_free}
%\end{figure}

%\begin{figure}[htb]
%	\begin{center}
%		\includegraphics[trim=2.5cm 1.5cm 2.5cm 1.5cm, scale=0.7] 			{inputs/pictures_ch3/cond_var_results_measurements_cfg6_int_saves}
%	\end{center}
%	\caption{Condition variable result LinuxRT} \label{fig_cond_var_result_linux}
%\end{figure}